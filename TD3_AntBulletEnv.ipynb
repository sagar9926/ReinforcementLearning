{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3Complete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagar9926/ReinforcementLearning/blob/master/TD3_AntBulletEnv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxW2eNW5LotA",
        "colab_type": "code",
        "outputId": "c4dab7c2-dfae-45e8-9b40-d748eaf279eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "!pip3 install pybullet --upgrade\n",
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pybullet in /usr/local/lib/python3.6/dist-packages (2.7.9)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Initializing the Experience Replay memory\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object): # since not inherriting from any other class thus passing object\n",
        "\n",
        "  def __init__(self, max_size=1000000): \n",
        "    self.storage = []                             # This is memory itself\n",
        "    self.max_size = max_size                      \n",
        "    self.ptr = 0                                  # index of different cells of memory, use to add transition to memory or do some sampling\n",
        "\n",
        "  def add(self, transition):                      # adds any new transition to the memory\n",
        "    if len(self.storage) == self.max_size:        # Check if memory has been fully populated\n",
        "      self.storage[int(self.ptr)] = transition    # if yes then the new transition should be added at the beginning\n",
        "      self.ptr = (self.ptr + 1) % self.max_size   #  Incrementing the pointer\n",
        "    else:\n",
        "      self.storage.append(transition)            # if memory is not fully populated , then just append the new transition \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def sample(self, batch_size):                 # Sample the transitions from memory and put them in the batch\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size) # generating some random indexes equal to batch size\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], [] \n",
        "    for i in ind: # iterate over the random indexes \n",
        "\n",
        "      state, next_state, action, reward, done = self.storage[i] # get the transition present at index i\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "      # before we convert batches into torch tensors we have to convert all of them to numpy array, Reshape converts the array into 1d array\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    #max action is to clip in case we added too much noise \n",
        "    super(Actor, self).__init__() # Activate the inherritance\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x)) #applying RELU breaks the non linearity\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) # Tanh returns the value between -1 and 1.By multiplying by max action we get continuous \n",
        "                                                      # values for actions within the permissible range \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # since we are going to use two critic model at the same time thus as defined below\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1) #output is one as we output single Q value\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1) # Concatenation of State and Action Vertically\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u): # this will be used for the step of gradient ascent of actor model. \n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object): # since not inherriting from any other class thus passing object as parameter\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action): # In this method we will create all our objects models and optimizers\n",
        "\n",
        "    #making sure our T3D class can work with any environment\n",
        "    # Create all our steps on device selected\n",
        "\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)  # Trained via GradientAscent\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device) # Trained vis Polyak Averaging\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict()) #load the actor target with the weights of the actor model\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters()) \n",
        "\n",
        "\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "\n",
        "\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device) #Converting to torch tensor 1D array\n",
        "    return self.actor(state).cpu().data.numpy().flatten() # Feeding the state to actor model to get the action\n",
        "    # Force the computation of forward pass on CPU and extract the data and convert to numpy and flatten\n",
        "    # the output to get 1D array\n",
        "\n",
        "    # Why numpy -> Torch -> Numpy\n",
        "    # At some point we need to use neural network for which we need pytorch format but we convert it back to numpy coz at some time we clip\n",
        "    # the action after adding noise to it and we are adding noise and clipping by numpy\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      # since we need to fed them to Neural Network thus convert them to Torch tensor\n",
        "      state = torch.Tensor(batch_states).to(device) # Will go in NN # This is still a batch of states\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "\n",
        "      #Generate the Gaussian noise for each of the elements of the batch of next actions\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)  # we create a noise tensor of size same size as batch actons \n",
        "      # Clipping the gaussian noise\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      # Adding noise and clippind the action \n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      # we are training the model for the entire episode\n",
        "      # min(Qt1, Qt2) this represents the value of the next state\n",
        "      # and if we are at the end of episode there is no next state and we have to start new episode\n",
        "      # therefore min(Qt1, Qt2) this doesnt apply any more\n",
        "      \n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0: # Delayed part of T3 DDPG model\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean() # Negative sign because we are performing gradient Ascend\n",
        "        # In above step we don't input here the current action played corresponding to state\n",
        "        # Differentiate wrt the actor parameters\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward() # Gradient Ascend step\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "2f1adacd-1902-4817-f955-055620e52249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "8595bd09-0599-4eca-e8cc-15ebddc07ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "d6d1761b-9f2b-46da-9866-95532319525e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "282a0e2c-0ab4-4bef-90e8-5bd508523092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  #print(\"total_timesteps:\",total_timesteps)\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "      #print(\"episode_timesteps:\",episode_timesteps)\n",
        "\n",
        "    #print(\"timesteps_since_eval:\",timesteps_since_eval)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 403.6110145579093\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 493.7248809865423\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 487.5458611799573\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: 494.7095981159783\n",
            "Total Timesteps: 4057 Episode Num: 5 Reward: 21.682947354397037\n",
            "Total Timesteps: 5057 Episode Num: 6 Reward: 516.3180482581204\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 99.659913\n",
            "---------------------------------------\n",
            "Total Timesteps: 6057 Episode Num: 7 Reward: 495.2739567488793\n",
            "Total Timesteps: 7057 Episode Num: 8 Reward: 498.3690502272596\n",
            "Total Timesteps: 8057 Episode Num: 9 Reward: 506.0940221056342\n",
            "Total Timesteps: 8163 Episode Num: 10 Reward: 47.702924899179216\n",
            "Total Timesteps: 9163 Episode Num: 11 Reward: 487.0410972981952\n",
            "Total Timesteps: 9464 Episode Num: 12 Reward: 135.6078814351959\n",
            "Total Timesteps: 10464 Episode Num: 13 Reward: 394.1240210627062\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 118.015300\n",
            "---------------------------------------\n",
            "Total Timesteps: 11464 Episode Num: 14 Reward: 113.45577648655613\n",
            "Total Timesteps: 12464 Episode Num: 15 Reward: 70.54948230539219\n",
            "Total Timesteps: 13464 Episode Num: 16 Reward: 95.83929942709969\n",
            "Total Timesteps: 14464 Episode Num: 17 Reward: 105.87605012275378\n",
            "Total Timesteps: 15464 Episode Num: 18 Reward: 91.86911785524676\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 82.260167\n",
            "---------------------------------------\n",
            "Total Timesteps: 16464 Episode Num: 19 Reward: 99.43889489496706\n",
            "Total Timesteps: 17464 Episode Num: 20 Reward: 213.37016695620505\n",
            "Total Timesteps: 18464 Episode Num: 21 Reward: 74.18267562511954\n",
            "Total Timesteps: 19464 Episode Num: 22 Reward: 72.17023498432566\n",
            "Total Timesteps: 20464 Episode Num: 23 Reward: 97.96130320591196\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 75.147697\n",
            "---------------------------------------\n",
            "Total Timesteps: 21464 Episode Num: 24 Reward: 100.50890765062363\n",
            "Total Timesteps: 22464 Episode Num: 25 Reward: 102.77584473114538\n",
            "Total Timesteps: 23464 Episode Num: 26 Reward: 99.28891873107783\n",
            "Total Timesteps: 24464 Episode Num: 27 Reward: 98.84122918386237\n",
            "Total Timesteps: 25464 Episode Num: 28 Reward: 93.55517202098696\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 20.613551\n",
            "---------------------------------------\n",
            "Total Timesteps: 26464 Episode Num: 29 Reward: 46.903628386420046\n",
            "Total Timesteps: 27464 Episode Num: 30 Reward: 67.26686281339285\n",
            "Total Timesteps: 28464 Episode Num: 31 Reward: 89.93225275943773\n",
            "Total Timesteps: 29464 Episode Num: 32 Reward: 97.08090583074478\n",
            "Total Timesteps: 30464 Episode Num: 33 Reward: 91.45319158469557\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 73.825868\n",
            "---------------------------------------\n",
            "Total Timesteps: 31464 Episode Num: 34 Reward: 188.20783505978892\n",
            "Total Timesteps: 32464 Episode Num: 35 Reward: 123.66786522568918\n",
            "Total Timesteps: 33464 Episode Num: 36 Reward: 248.73988761622232\n",
            "Total Timesteps: 34464 Episode Num: 37 Reward: 441.68727095401573\n",
            "Total Timesteps: 35464 Episode Num: 38 Reward: 502.86091010238255\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 186.405786\n",
            "---------------------------------------\n",
            "Total Timesteps: 36464 Episode Num: 39 Reward: 390.7963694402669\n",
            "Total Timesteps: 37464 Episode Num: 40 Reward: 384.30019693123467\n",
            "Total Timesteps: 38464 Episode Num: 41 Reward: 387.2792694926729\n",
            "Total Timesteps: 39464 Episode Num: 42 Reward: 210.8120392390885\n",
            "Total Timesteps: 40464 Episode Num: 43 Reward: 427.1761345247209\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 223.371164\n",
            "---------------------------------------\n",
            "Total Timesteps: 41464 Episode Num: 44 Reward: 176.09716102881228\n",
            "Total Timesteps: 42464 Episode Num: 45 Reward: 230.89529812600327\n",
            "Total Timesteps: 43464 Episode Num: 46 Reward: 226.97000644746282\n",
            "Total Timesteps: 44464 Episode Num: 47 Reward: 95.84460844961379\n",
            "Total Timesteps: 45464 Episode Num: 48 Reward: 213.17249236543086\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 211.064762\n",
            "---------------------------------------\n",
            "Total Timesteps: 46464 Episode Num: 49 Reward: 309.83347552758386\n",
            "Total Timesteps: 47464 Episode Num: 50 Reward: 121.99515761687617\n",
            "Total Timesteps: 48464 Episode Num: 51 Reward: 138.82354585340917\n",
            "Total Timesteps: 49464 Episode Num: 52 Reward: 304.7549839372112\n",
            "Total Timesteps: 50464 Episode Num: 53 Reward: 75.85936474516188\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 262.020093\n",
            "---------------------------------------\n",
            "Total Timesteps: 51464 Episode Num: 54 Reward: 291.9794395722094\n",
            "Total Timesteps: 52464 Episode Num: 55 Reward: 293.0804462454381\n",
            "Total Timesteps: 53464 Episode Num: 56 Reward: 194.75093190123536\n",
            "Total Timesteps: 54464 Episode Num: 57 Reward: 195.74181155396576\n",
            "Total Timesteps: 55464 Episode Num: 58 Reward: 262.43837427011744\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 132.193017\n",
            "---------------------------------------\n",
            "Total Timesteps: 56464 Episode Num: 59 Reward: 84.14939197514819\n",
            "Total Timesteps: 57464 Episode Num: 60 Reward: 322.682783206081\n",
            "Total Timesteps: 58464 Episode Num: 61 Reward: 223.90063850425344\n",
            "Total Timesteps: 59464 Episode Num: 62 Reward: 403.03066820862176\n",
            "Total Timesteps: 60464 Episode Num: 63 Reward: 90.22798855494467\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 71.512739\n",
            "---------------------------------------\n",
            "Total Timesteps: 60630 Episode Num: 64 Reward: 78.75657186467963\n",
            "Total Timesteps: 61630 Episode Num: 65 Reward: 418.2512029994688\n",
            "Total Timesteps: 62630 Episode Num: 66 Reward: 219.32854999213225\n",
            "Total Timesteps: 63630 Episode Num: 67 Reward: 82.0913346567622\n",
            "Total Timesteps: 64630 Episode Num: 68 Reward: 277.7475626227287\n",
            "Total Timesteps: 65630 Episode Num: 69 Reward: 226.11891511643938\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 216.911997\n",
            "---------------------------------------\n",
            "Total Timesteps: 66630 Episode Num: 70 Reward: 99.2293503582703\n",
            "Total Timesteps: 67630 Episode Num: 71 Reward: 217.24850359446143\n",
            "Total Timesteps: 68630 Episode Num: 72 Reward: 162.75114039151444\n",
            "Total Timesteps: 69630 Episode Num: 73 Reward: 172.32614673641032\n",
            "Total Timesteps: 70630 Episode Num: 74 Reward: 242.485763554409\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 84.586303\n",
            "---------------------------------------\n",
            "Total Timesteps: 71630 Episode Num: 75 Reward: 90.9972164786919\n",
            "Total Timesteps: 72630 Episode Num: 76 Reward: 84.18377955965411\n",
            "Total Timesteps: 73630 Episode Num: 77 Reward: 129.09447939432667\n",
            "Total Timesteps: 74630 Episode Num: 78 Reward: 313.53963484783543\n",
            "Total Timesteps: 74650 Episode Num: 79 Reward: -0.8555788194575471\n",
            "Total Timesteps: 74670 Episode Num: 80 Reward: -0.7637972356989544\n",
            "Total Timesteps: 74690 Episode Num: 81 Reward: -1.4534460737727386\n",
            "Total Timesteps: 74710 Episode Num: 82 Reward: -0.45175759560692397\n",
            "Total Timesteps: 74730 Episode Num: 83 Reward: 0.026998013619447647\n",
            "Total Timesteps: 74750 Episode Num: 84 Reward: -0.1794286199233417\n",
            "Total Timesteps: 74770 Episode Num: 85 Reward: -0.3048765530030981\n",
            "Total Timesteps: 74790 Episode Num: 86 Reward: -0.30629183130640136\n",
            "Total Timesteps: 74810 Episode Num: 87 Reward: -0.16812140168842582\n",
            "Total Timesteps: 74830 Episode Num: 88 Reward: -0.4236502534847628\n",
            "Total Timesteps: 74850 Episode Num: 89 Reward: -0.4178345815200024\n",
            "Total Timesteps: 74870 Episode Num: 90 Reward: -0.7166115014955419\n",
            "Total Timesteps: 74890 Episode Num: 91 Reward: -0.9196994271037968\n",
            "Total Timesteps: 74910 Episode Num: 92 Reward: -0.4436602082947054\n",
            "Total Timesteps: 74930 Episode Num: 93 Reward: -0.3856969656491911\n",
            "Total Timesteps: 74950 Episode Num: 94 Reward: -0.4096226399291858\n",
            "Total Timesteps: 74970 Episode Num: 95 Reward: -0.1896238161408168\n",
            "Total Timesteps: 74990 Episode Num: 96 Reward: -1.2092339503866003\n",
            "Total Timesteps: 75010 Episode Num: 97 Reward: -0.7161584979269704\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -0.997746\n",
            "---------------------------------------\n",
            "Total Timesteps: 75030 Episode Num: 98 Reward: -0.6671593991086897\n",
            "Total Timesteps: 75050 Episode Num: 99 Reward: -0.2766088826190294\n",
            "Total Timesteps: 75070 Episode Num: 100 Reward: 0.4694141783642891\n",
            "Total Timesteps: 75090 Episode Num: 101 Reward: 0.15508376254119405\n",
            "Total Timesteps: 75110 Episode Num: 102 Reward: -0.6636803697340352\n",
            "Total Timesteps: 75130 Episode Num: 103 Reward: -0.9403939908115166\n",
            "Total Timesteps: 75150 Episode Num: 104 Reward: 0.21985720909891038\n",
            "Total Timesteps: 75170 Episode Num: 105 Reward: -0.7879615560259512\n",
            "Total Timesteps: 75190 Episode Num: 106 Reward: -0.15530042064539673\n",
            "Total Timesteps: 75210 Episode Num: 107 Reward: -0.41530985543828214\n",
            "Total Timesteps: 75230 Episode Num: 108 Reward: -1.1320346959897747\n",
            "Total Timesteps: 75250 Episode Num: 109 Reward: -0.9482351141897771\n",
            "Total Timesteps: 75283 Episode Num: 110 Reward: -0.7561410149931669\n",
            "Total Timesteps: 75303 Episode Num: 111 Reward: -0.7660476186352168\n",
            "Total Timesteps: 75451 Episode Num: 112 Reward: 38.70442808372801\n",
            "Total Timesteps: 75572 Episode Num: 113 Reward: 34.7283591383311\n",
            "Total Timesteps: 76572 Episode Num: 114 Reward: 197.93618398283854\n",
            "Total Timesteps: 77572 Episode Num: 115 Reward: 191.96993720209358\n",
            "Total Timesteps: 78572 Episode Num: 116 Reward: 259.4072541144968\n",
            "Total Timesteps: 78692 Episode Num: 117 Reward: 25.36120496284608\n",
            "Total Timesteps: 79692 Episode Num: 118 Reward: 248.6099579211497\n",
            "Total Timesteps: 80692 Episode Num: 119 Reward: 318.060388318442\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 132.725249\n",
            "---------------------------------------\n",
            "Total Timesteps: 81692 Episode Num: 120 Reward: 447.2361557804815\n",
            "Total Timesteps: 82344 Episode Num: 121 Reward: 214.45384368461737\n",
            "Total Timesteps: 83344 Episode Num: 122 Reward: 270.6176405182244\n",
            "Total Timesteps: 84344 Episode Num: 123 Reward: 204.55362099528128\n",
            "Total Timesteps: 84424 Episode Num: 124 Reward: 8.626937062355672\n",
            "Total Timesteps: 85424 Episode Num: 125 Reward: 124.75468069583268\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 182.213114\n",
            "---------------------------------------\n",
            "Total Timesteps: 86424 Episode Num: 126 Reward: 236.45222162624415\n",
            "Total Timesteps: 87108 Episode Num: 127 Reward: 297.0887976224642\n",
            "Total Timesteps: 88108 Episode Num: 128 Reward: 435.67324679208764\n",
            "Total Timesteps: 88700 Episode Num: 129 Reward: 173.26125601691845\n",
            "Total Timesteps: 89700 Episode Num: 130 Reward: 247.97253678984285\n",
            "Total Timesteps: 90700 Episode Num: 131 Reward: 103.58203701179049\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 168.972597\n",
            "---------------------------------------\n",
            "Total Timesteps: 91700 Episode Num: 132 Reward: 209.65801862109475\n",
            "Total Timesteps: 92700 Episode Num: 133 Reward: 342.3283723340606\n",
            "Total Timesteps: 93700 Episode Num: 134 Reward: 387.7397787114092\n",
            "Total Timesteps: 94700 Episode Num: 135 Reward: 310.49810795259185\n",
            "Total Timesteps: 95700 Episode Num: 136 Reward: 107.22856914790893\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 316.252923\n",
            "---------------------------------------\n",
            "Total Timesteps: 96700 Episode Num: 137 Reward: 334.79127501673213\n",
            "Total Timesteps: 97700 Episode Num: 138 Reward: 332.16262677431877\n",
            "Total Timesteps: 98700 Episode Num: 139 Reward: 287.54947461235446\n",
            "Total Timesteps: 98801 Episode Num: 140 Reward: -18.979743475927002\n",
            "Total Timesteps: 99801 Episode Num: 141 Reward: 206.56519143675905\n",
            "Total Timesteps: 100801 Episode Num: 142 Reward: 199.15534233027412\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 358.486641\n",
            "---------------------------------------\n",
            "Total Timesteps: 101801 Episode Num: 143 Reward: 384.6013911934325\n",
            "Total Timesteps: 102801 Episode Num: 144 Reward: 318.63750521358656\n",
            "Total Timesteps: 103801 Episode Num: 145 Reward: 311.1212105752296\n",
            "Total Timesteps: 104801 Episode Num: 146 Reward: 129.49456811333914\n",
            "Total Timesteps: 105801 Episode Num: 147 Reward: 433.7178487071047\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 281.061603\n",
            "---------------------------------------\n",
            "Total Timesteps: 106801 Episode Num: 148 Reward: 87.931287207969\n",
            "Total Timesteps: 107801 Episode Num: 149 Reward: 478.183138443515\n",
            "Total Timesteps: 108801 Episode Num: 150 Reward: 450.08602487061063\n",
            "Total Timesteps: 109801 Episode Num: 151 Reward: 348.21313365161996\n",
            "Total Timesteps: 110801 Episode Num: 152 Reward: 234.34591104980336\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 356.959093\n",
            "---------------------------------------\n",
            "Total Timesteps: 111801 Episode Num: 153 Reward: 303.7788792186521\n",
            "Total Timesteps: 112801 Episode Num: 154 Reward: 321.3684200326308\n",
            "Total Timesteps: 113801 Episode Num: 155 Reward: 335.49342624324686\n",
            "Total Timesteps: 114801 Episode Num: 156 Reward: 343.62504790734823\n",
            "Total Timesteps: 115801 Episode Num: 157 Reward: 309.3946025645093\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 351.833247\n",
            "---------------------------------------\n",
            "Total Timesteps: 116801 Episode Num: 158 Reward: 358.26487833752935\n",
            "Total Timesteps: 117801 Episode Num: 159 Reward: 416.83123535521054\n",
            "Total Timesteps: 118801 Episode Num: 160 Reward: 435.5353034450912\n",
            "Total Timesteps: 119801 Episode Num: 161 Reward: 448.88413332042603\n",
            "Total Timesteps: 120801 Episode Num: 162 Reward: 333.4717082122256\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 306.307110\n",
            "---------------------------------------\n",
            "Total Timesteps: 121801 Episode Num: 163 Reward: 629.8644855494125\n",
            "Total Timesteps: 122801 Episode Num: 164 Reward: 267.2267305815879\n",
            "Total Timesteps: 123801 Episode Num: 165 Reward: 461.85974663941465\n",
            "Total Timesteps: 124801 Episode Num: 166 Reward: 402.80416404469355\n",
            "Total Timesteps: 125801 Episode Num: 167 Reward: 253.59440926239668\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 399.285371\n",
            "---------------------------------------\n",
            "Total Timesteps: 126801 Episode Num: 168 Reward: 428.66036765400264\n",
            "Total Timesteps: 127801 Episode Num: 169 Reward: 356.1116899376191\n",
            "Total Timesteps: 128801 Episode Num: 170 Reward: 547.4387495409655\n",
            "Total Timesteps: 129801 Episode Num: 171 Reward: 488.1647196241873\n",
            "Total Timesteps: 130801 Episode Num: 172 Reward: 156.84456673213248\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 352.885970\n",
            "---------------------------------------\n",
            "Total Timesteps: 130945 Episode Num: 173 Reward: 96.74030364508096\n",
            "Total Timesteps: 131945 Episode Num: 174 Reward: 505.67728304142247\n",
            "Total Timesteps: 132945 Episode Num: 175 Reward: 641.7927572704064\n",
            "Total Timesteps: 133945 Episode Num: 176 Reward: 374.7266364867537\n",
            "Total Timesteps: 134945 Episode Num: 177 Reward: 386.0674171121829\n",
            "Total Timesteps: 135945 Episode Num: 178 Reward: 696.9899279947358\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 482.004915\n",
            "---------------------------------------\n",
            "Total Timesteps: 136945 Episode Num: 179 Reward: 647.0628854337181\n",
            "Total Timesteps: 137945 Episode Num: 180 Reward: 536.8649073662465\n",
            "Total Timesteps: 138945 Episode Num: 181 Reward: 320.0901637311696\n",
            "Total Timesteps: 139945 Episode Num: 182 Reward: 675.3562501599024\n",
            "Total Timesteps: 140945 Episode Num: 183 Reward: 715.0810227562449\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 568.433969\n",
            "---------------------------------------\n",
            "Total Timesteps: 141945 Episode Num: 184 Reward: 630.8567394636052\n",
            "Total Timesteps: 142945 Episode Num: 185 Reward: 569.8104381102444\n",
            "Total Timesteps: 143945 Episode Num: 186 Reward: 577.0333593583455\n",
            "Total Timesteps: 144945 Episode Num: 187 Reward: 663.3310939495981\n",
            "Total Timesteps: 145945 Episode Num: 188 Reward: 534.3116799030371\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 532.187088\n",
            "---------------------------------------\n",
            "Total Timesteps: 146945 Episode Num: 189 Reward: 553.3038149535801\n",
            "Total Timesteps: 147945 Episode Num: 190 Reward: 571.9271225906333\n",
            "Total Timesteps: 148945 Episode Num: 191 Reward: 455.63833835168066\n",
            "Total Timesteps: 149945 Episode Num: 192 Reward: 526.9077571203128\n",
            "Total Timesteps: 150945 Episode Num: 193 Reward: 356.00244394740764\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 510.728568\n",
            "---------------------------------------\n",
            "Total Timesteps: 151945 Episode Num: 194 Reward: 451.35463454685896\n",
            "Total Timesteps: 152945 Episode Num: 195 Reward: 510.3303829862768\n",
            "Total Timesteps: 153945 Episode Num: 196 Reward: 321.72565049600524\n",
            "Total Timesteps: 154945 Episode Num: 197 Reward: 433.6801351895242\n",
            "Total Timesteps: 155945 Episode Num: 198 Reward: 452.2323935753257\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 402.532983\n",
            "---------------------------------------\n",
            "Total Timesteps: 156945 Episode Num: 199 Reward: 461.0061851736828\n",
            "Total Timesteps: 157048 Episode Num: 200 Reward: -40.130808455764175\n",
            "Total Timesteps: 158048 Episode Num: 201 Reward: 545.9035556271665\n",
            "Total Timesteps: 159048 Episode Num: 202 Reward: 410.2156897746501\n",
            "Total Timesteps: 160048 Episode Num: 203 Reward: 615.831541179301\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 391.002892\n",
            "---------------------------------------\n",
            "Total Timesteps: 161048 Episode Num: 204 Reward: 488.2785843795786\n",
            "Total Timesteps: 162048 Episode Num: 205 Reward: 217.63364074637494\n",
            "Total Timesteps: 163048 Episode Num: 206 Reward: 262.5683526400655\n",
            "Total Timesteps: 164048 Episode Num: 207 Reward: 282.86285047955977\n",
            "Total Timesteps: 165048 Episode Num: 208 Reward: 456.9656016715235\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 459.298236\n",
            "---------------------------------------\n",
            "Total Timesteps: 166048 Episode Num: 209 Reward: 557.6756004236657\n",
            "Total Timesteps: 167048 Episode Num: 210 Reward: 526.8672852893017\n",
            "Total Timesteps: 168048 Episode Num: 211 Reward: 591.6367668279123\n",
            "Total Timesteps: 169048 Episode Num: 212 Reward: 454.97724492300307\n",
            "Total Timesteps: 170048 Episode Num: 213 Reward: 394.0877060723387\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 470.918290\n",
            "---------------------------------------\n",
            "Total Timesteps: 171048 Episode Num: 214 Reward: 489.03355697668775\n",
            "Total Timesteps: 172048 Episode Num: 215 Reward: 402.42366158488517\n",
            "Total Timesteps: 173048 Episode Num: 216 Reward: 668.5134788470438\n",
            "Total Timesteps: 174048 Episode Num: 217 Reward: 347.46673875053347\n",
            "Total Timesteps: 175048 Episode Num: 218 Reward: 392.4604478354471\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 414.332240\n",
            "---------------------------------------\n",
            "Total Timesteps: 176048 Episode Num: 219 Reward: 336.200762490527\n",
            "Total Timesteps: 177048 Episode Num: 220 Reward: 616.0580414660994\n",
            "Total Timesteps: 178048 Episode Num: 221 Reward: 800.0332533844362\n",
            "Total Timesteps: 179048 Episode Num: 222 Reward: 726.3815263711273\n",
            "Total Timesteps: 180048 Episode Num: 223 Reward: 751.122354805214\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 542.434550\n",
            "---------------------------------------\n",
            "Total Timesteps: 181048 Episode Num: 224 Reward: 722.847536845935\n",
            "Total Timesteps: 182048 Episode Num: 225 Reward: 630.6909121823624\n",
            "Total Timesteps: 183048 Episode Num: 226 Reward: 557.245716553537\n",
            "Total Timesteps: 184048 Episode Num: 227 Reward: 455.3739262190651\n",
            "Total Timesteps: 184532 Episode Num: 228 Reward: 239.24386978652205\n",
            "Total Timesteps: 185532 Episode Num: 229 Reward: 526.3304204817152\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 634.852614\n",
            "---------------------------------------\n",
            "Total Timesteps: 186532 Episode Num: 230 Reward: 634.6043588377664\n",
            "Total Timesteps: 187532 Episode Num: 231 Reward: 679.2674039921798\n",
            "Total Timesteps: 188532 Episode Num: 232 Reward: 709.7231100616125\n",
            "Total Timesteps: 188553 Episode Num: 233 Reward: 3.683319588586616\n",
            "Total Timesteps: 189553 Episode Num: 234 Reward: 571.3092534266135\n",
            "Total Timesteps: 190553 Episode Num: 235 Reward: 592.758281233986\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 563.281199\n",
            "---------------------------------------\n",
            "Total Timesteps: 191553 Episode Num: 236 Reward: 541.9278671576149\n",
            "Total Timesteps: 191935 Episode Num: 237 Reward: 218.68537049498568\n",
            "Total Timesteps: 192935 Episode Num: 238 Reward: 774.5218662732774\n",
            "Total Timesteps: 193935 Episode Num: 239 Reward: 571.4901240668142\n",
            "Total Timesteps: 194935 Episode Num: 240 Reward: 631.1047578228398\n",
            "Total Timesteps: 195935 Episode Num: 241 Reward: 561.6898395188042\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 695.054292\n",
            "---------------------------------------\n",
            "Total Timesteps: 196935 Episode Num: 242 Reward: 822.6182772783174\n",
            "Total Timesteps: 197935 Episode Num: 243 Reward: 698.682045428282\n",
            "Total Timesteps: 198935 Episode Num: 244 Reward: 727.1417678459089\n",
            "Total Timesteps: 199935 Episode Num: 245 Reward: 572.7059330331961\n",
            "Total Timesteps: 200935 Episode Num: 246 Reward: 683.8513386675237\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 487.809776\n",
            "---------------------------------------\n",
            "Total Timesteps: 201935 Episode Num: 247 Reward: 472.5716687440728\n",
            "Total Timesteps: 202935 Episode Num: 248 Reward: 666.99039014182\n",
            "Total Timesteps: 203935 Episode Num: 249 Reward: 482.2201347730958\n",
            "Total Timesteps: 204935 Episode Num: 250 Reward: 318.64745027124013\n",
            "Total Timesteps: 205935 Episode Num: 251 Reward: 625.0566305437777\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 470.244719\n",
            "---------------------------------------\n",
            "Total Timesteps: 206935 Episode Num: 252 Reward: 594.039863352887\n",
            "Total Timesteps: 207935 Episode Num: 253 Reward: 625.8787335948716\n",
            "Total Timesteps: 208935 Episode Num: 254 Reward: 626.494175204073\n",
            "Total Timesteps: 209935 Episode Num: 255 Reward: 324.4728997505002\n",
            "Total Timesteps: 210935 Episode Num: 256 Reward: 557.5171054201262\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 603.989994\n",
            "---------------------------------------\n",
            "Total Timesteps: 211935 Episode Num: 257 Reward: 499.25989001080876\n",
            "Total Timesteps: 212935 Episode Num: 258 Reward: 709.4608396047057\n",
            "Total Timesteps: 213935 Episode Num: 259 Reward: 342.5313000134484\n",
            "Total Timesteps: 214935 Episode Num: 260 Reward: 675.7061619208057\n",
            "Total Timesteps: 215935 Episode Num: 261 Reward: 678.3992086826167\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 711.276403\n",
            "---------------------------------------\n",
            "Total Timesteps: 216935 Episode Num: 262 Reward: 682.8877372384483\n",
            "Total Timesteps: 217935 Episode Num: 263 Reward: 813.1906756750419\n",
            "Total Timesteps: 218096 Episode Num: 264 Reward: 4.284345796359002\n",
            "Total Timesteps: 219096 Episode Num: 265 Reward: 678.4701569041573\n",
            "Total Timesteps: 220096 Episode Num: 266 Reward: 497.69232097526026\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 339.815009\n",
            "---------------------------------------\n",
            "Total Timesteps: 220256 Episode Num: 267 Reward: -20.97650965200008\n",
            "Total Timesteps: 221256 Episode Num: 268 Reward: 548.0169304477572\n",
            "Total Timesteps: 222256 Episode Num: 269 Reward: 497.36601127203556\n",
            "Total Timesteps: 223256 Episode Num: 270 Reward: 588.9943606950351\n",
            "Total Timesteps: 224256 Episode Num: 271 Reward: 586.977156343958\n",
            "Total Timesteps: 225256 Episode Num: 272 Reward: 636.320768780975\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 593.997125\n",
            "---------------------------------------\n",
            "Total Timesteps: 226256 Episode Num: 273 Reward: 708.5450287719755\n",
            "Total Timesteps: 227256 Episode Num: 274 Reward: 560.3966064759502\n",
            "Total Timesteps: 228256 Episode Num: 275 Reward: 557.8758227333892\n",
            "Total Timesteps: 229256 Episode Num: 276 Reward: 616.2713153794538\n",
            "Total Timesteps: 230256 Episode Num: 277 Reward: 523.4030834573334\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 536.409836\n",
            "---------------------------------------\n",
            "Total Timesteps: 231256 Episode Num: 278 Reward: 355.2300084517302\n",
            "Total Timesteps: 232256 Episode Num: 279 Reward: 601.8407767200018\n",
            "Total Timesteps: 233256 Episode Num: 280 Reward: 434.53279115752787\n",
            "Total Timesteps: 234256 Episode Num: 281 Reward: 155.9084080155754\n",
            "Total Timesteps: 235256 Episode Num: 282 Reward: 489.6891372048464\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 413.934346\n",
            "---------------------------------------\n",
            "Total Timesteps: 236256 Episode Num: 283 Reward: 352.76173987256044\n",
            "Total Timesteps: 237256 Episode Num: 284 Reward: 568.8326753283807\n",
            "Total Timesteps: 238256 Episode Num: 285 Reward: 464.0632850980327\n",
            "Total Timesteps: 238407 Episode Num: 286 Reward: 76.92725010215969\n",
            "Total Timesteps: 239407 Episode Num: 287 Reward: 646.7783546330642\n",
            "Total Timesteps: 240407 Episode Num: 288 Reward: 491.40265929080175\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 517.708467\n",
            "---------------------------------------\n",
            "Total Timesteps: 241407 Episode Num: 289 Reward: 511.45734666140845\n",
            "Total Timesteps: 242407 Episode Num: 290 Reward: 359.0107682145145\n",
            "Total Timesteps: 243407 Episode Num: 291 Reward: 609.042661477687\n",
            "Total Timesteps: 244407 Episode Num: 292 Reward: 527.3531148890905\n",
            "Total Timesteps: 245407 Episode Num: 293 Reward: 459.10502294817974\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 557.800785\n",
            "---------------------------------------\n",
            "Total Timesteps: 246407 Episode Num: 294 Reward: 717.4055784246738\n",
            "Total Timesteps: 247407 Episode Num: 295 Reward: 519.9818438578907\n",
            "Total Timesteps: 248407 Episode Num: 296 Reward: 722.1585832287524\n",
            "Total Timesteps: 249407 Episode Num: 297 Reward: 442.77696615588957\n",
            "Total Timesteps: 250407 Episode Num: 298 Reward: 635.2704618590053\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 708.463685\n",
            "---------------------------------------\n",
            "Total Timesteps: 251407 Episode Num: 299 Reward: 355.1272963894498\n",
            "Total Timesteps: 252407 Episode Num: 300 Reward: 608.7944598063916\n",
            "Total Timesteps: 253407 Episode Num: 301 Reward: 652.6878292688773\n",
            "Total Timesteps: 254407 Episode Num: 302 Reward: 853.7335260958844\n",
            "Total Timesteps: 255407 Episode Num: 303 Reward: 760.0524530390652\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 530.762479\n",
            "---------------------------------------\n",
            "Total Timesteps: 256407 Episode Num: 304 Reward: 558.5014854164575\n",
            "Total Timesteps: 257407 Episode Num: 305 Reward: 437.6520650957946\n",
            "Total Timesteps: 258407 Episode Num: 306 Reward: 442.25071307888544\n",
            "Total Timesteps: 259407 Episode Num: 307 Reward: 491.44341160233256\n",
            "Total Timesteps: 260407 Episode Num: 308 Reward: 268.5949385998758\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 701.375739\n",
            "---------------------------------------\n",
            "Total Timesteps: 261407 Episode Num: 309 Reward: 672.3164456363982\n",
            "Total Timesteps: 262407 Episode Num: 310 Reward: 481.0726771883965\n",
            "Total Timesteps: 263407 Episode Num: 311 Reward: 870.8109991645051\n",
            "Total Timesteps: 264407 Episode Num: 312 Reward: 473.05823152402553\n",
            "Total Timesteps: 265407 Episode Num: 313 Reward: 740.7262370591836\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 555.530031\n",
            "---------------------------------------\n",
            "Total Timesteps: 266407 Episode Num: 314 Reward: 470.97308236009627\n",
            "Total Timesteps: 267407 Episode Num: 315 Reward: 594.1930688783018\n",
            "Total Timesteps: 268407 Episode Num: 316 Reward: 752.7203496632594\n",
            "Total Timesteps: 268736 Episode Num: 317 Reward: 227.4043361174862\n",
            "Total Timesteps: 269585 Episode Num: 318 Reward: 551.3831605668604\n",
            "Total Timesteps: 270585 Episode Num: 319 Reward: 722.7635515068657\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 617.565790\n",
            "---------------------------------------\n",
            "Total Timesteps: 271585 Episode Num: 320 Reward: 566.494028880928\n",
            "Total Timesteps: 272585 Episode Num: 321 Reward: 499.0930139805099\n",
            "Total Timesteps: 273585 Episode Num: 322 Reward: 702.0459142234982\n",
            "Total Timesteps: 274585 Episode Num: 323 Reward: 575.3671606830455\n",
            "Total Timesteps: 275585 Episode Num: 324 Reward: 427.04855457755565\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 648.598603\n",
            "---------------------------------------\n",
            "Total Timesteps: 276585 Episode Num: 325 Reward: 607.858958998101\n",
            "Total Timesteps: 277585 Episode Num: 326 Reward: 773.2407253459772\n",
            "Total Timesteps: 278585 Episode Num: 327 Reward: 787.8014569177715\n",
            "Total Timesteps: 279585 Episode Num: 328 Reward: 656.2157861950245\n",
            "Total Timesteps: 280585 Episode Num: 329 Reward: 680.3391266156436\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 713.087574\n",
            "---------------------------------------\n",
            "Total Timesteps: 281585 Episode Num: 330 Reward: 619.125558937935\n",
            "Total Timesteps: 282585 Episode Num: 331 Reward: 672.1559737551898\n",
            "Total Timesteps: 283585 Episode Num: 332 Reward: 744.9333919792713\n",
            "Total Timesteps: 284585 Episode Num: 333 Reward: 596.593454220351\n",
            "Total Timesteps: 285585 Episode Num: 334 Reward: 664.9914052096208\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 632.869657\n",
            "---------------------------------------\n",
            "Total Timesteps: 286585 Episode Num: 335 Reward: 579.4784978934612\n",
            "Total Timesteps: 287585 Episode Num: 336 Reward: 793.5445784487544\n",
            "Total Timesteps: 288585 Episode Num: 337 Reward: 733.606951058168\n",
            "Total Timesteps: 289585 Episode Num: 338 Reward: 423.35554421999115\n",
            "Total Timesteps: 290585 Episode Num: 339 Reward: 796.0705640124024\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 512.918050\n",
            "---------------------------------------\n",
            "Total Timesteps: 291585 Episode Num: 340 Reward: 473.74555075376765\n",
            "Total Timesteps: 292585 Episode Num: 341 Reward: 836.9194090023625\n",
            "Total Timesteps: 293585 Episode Num: 342 Reward: 563.1021244682106\n",
            "Total Timesteps: 294585 Episode Num: 343 Reward: 687.3738035156471\n",
            "Total Timesteps: 295585 Episode Num: 344 Reward: 818.9132965827843\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 606.798046\n",
            "---------------------------------------\n",
            "Total Timesteps: 296585 Episode Num: 345 Reward: 701.3441519231892\n",
            "Total Timesteps: 297585 Episode Num: 346 Reward: 793.490802450451\n",
            "Total Timesteps: 298585 Episode Num: 347 Reward: 710.3402972177408\n",
            "Total Timesteps: 299585 Episode Num: 348 Reward: 831.6714759267084\n",
            "Total Timesteps: 300585 Episode Num: 349 Reward: 884.3391645349786\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 812.782852\n",
            "---------------------------------------\n",
            "Total Timesteps: 301585 Episode Num: 350 Reward: 868.9124293707298\n",
            "Total Timesteps: 302585 Episode Num: 351 Reward: 829.1376783921206\n",
            "Total Timesteps: 303585 Episode Num: 352 Reward: 806.7489699338773\n",
            "Total Timesteps: 304585 Episode Num: 353 Reward: 811.7423585075775\n",
            "Total Timesteps: 305585 Episode Num: 354 Reward: 683.4102807379503\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 869.075596\n",
            "---------------------------------------\n",
            "Total Timesteps: 306585 Episode Num: 355 Reward: 852.6393104557831\n",
            "Total Timesteps: 307585 Episode Num: 356 Reward: 733.6897710695306\n",
            "Total Timesteps: 308585 Episode Num: 357 Reward: 672.6416544247589\n",
            "Total Timesteps: 309585 Episode Num: 358 Reward: 759.4025752718285\n",
            "Total Timesteps: 310585 Episode Num: 359 Reward: 693.1561435831774\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 846.351726\n",
            "---------------------------------------\n",
            "Total Timesteps: 311585 Episode Num: 360 Reward: 838.4811178297955\n",
            "Total Timesteps: 312585 Episode Num: 361 Reward: 817.2398786172469\n",
            "Total Timesteps: 313585 Episode Num: 362 Reward: 789.5241628773136\n",
            "Total Timesteps: 314585 Episode Num: 363 Reward: 554.987549721171\n",
            "Total Timesteps: 315585 Episode Num: 364 Reward: 705.5860181743014\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 764.689950\n",
            "---------------------------------------\n",
            "Total Timesteps: 316585 Episode Num: 365 Reward: 892.9645750270962\n",
            "Total Timesteps: 317585 Episode Num: 366 Reward: 820.8789531190293\n",
            "Total Timesteps: 318585 Episode Num: 367 Reward: 777.8757918624697\n",
            "Total Timesteps: 319585 Episode Num: 368 Reward: 787.0918403403473\n",
            "Total Timesteps: 320585 Episode Num: 369 Reward: 674.5837652443339\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 732.864070\n",
            "---------------------------------------\n",
            "Total Timesteps: 321585 Episode Num: 370 Reward: 629.6653118372618\n",
            "Total Timesteps: 322585 Episode Num: 371 Reward: 787.5537064475991\n",
            "Total Timesteps: 323585 Episode Num: 372 Reward: 732.1500340062105\n",
            "Total Timesteps: 324585 Episode Num: 373 Reward: 783.0908412912294\n",
            "Total Timesteps: 325585 Episode Num: 374 Reward: 735.4513142380438\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 796.710139\n",
            "---------------------------------------\n",
            "Total Timesteps: 326585 Episode Num: 375 Reward: 820.6484730027485\n",
            "Total Timesteps: 327585 Episode Num: 376 Reward: 762.8340511262486\n",
            "Total Timesteps: 328585 Episode Num: 377 Reward: 726.5280264408718\n",
            "Total Timesteps: 329585 Episode Num: 378 Reward: 844.2946650325609\n",
            "Total Timesteps: 330585 Episode Num: 379 Reward: 819.5793255621287\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 742.581734\n",
            "---------------------------------------\n",
            "Total Timesteps: 331585 Episode Num: 380 Reward: 682.127696553854\n",
            "Total Timesteps: 332585 Episode Num: 381 Reward: 842.4372070387333\n",
            "Total Timesteps: 333585 Episode Num: 382 Reward: 977.6359576594249\n",
            "Total Timesteps: 334585 Episode Num: 383 Reward: 781.4528580591208\n",
            "Total Timesteps: 335585 Episode Num: 384 Reward: 977.5688071245914\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 862.746213\n",
            "---------------------------------------\n",
            "Total Timesteps: 336585 Episode Num: 385 Reward: 942.617566865377\n",
            "Total Timesteps: 337585 Episode Num: 386 Reward: 1162.5073566732597\n",
            "Total Timesteps: 338585 Episode Num: 387 Reward: 1152.718797050696\n",
            "Total Timesteps: 339585 Episode Num: 388 Reward: 915.9196967245589\n",
            "Total Timesteps: 340585 Episode Num: 389 Reward: 1143.7966583705322\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 828.314651\n",
            "---------------------------------------\n",
            "Total Timesteps: 341585 Episode Num: 390 Reward: 788.8612934452736\n",
            "Total Timesteps: 342585 Episode Num: 391 Reward: 841.47953998018\n",
            "Total Timesteps: 343585 Episode Num: 392 Reward: 840.2040619618494\n",
            "Total Timesteps: 344585 Episode Num: 393 Reward: 964.2439318414022\n",
            "Total Timesteps: 345585 Episode Num: 394 Reward: 508.60189448537193\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 530.772752\n",
            "---------------------------------------\n",
            "Total Timesteps: 346585 Episode Num: 395 Reward: 515.000415959289\n",
            "Total Timesteps: 347585 Episode Num: 396 Reward: 1119.2748500268249\n",
            "Total Timesteps: 348585 Episode Num: 397 Reward: 956.7656158194245\n",
            "Total Timesteps: 349585 Episode Num: 398 Reward: 677.4083616624582\n",
            "Total Timesteps: 350585 Episode Num: 399 Reward: 702.889999793949\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 713.687894\n",
            "---------------------------------------\n",
            "Total Timesteps: 351585 Episode Num: 400 Reward: 848.830617129094\n",
            "Total Timesteps: 352585 Episode Num: 401 Reward: 624.6414387799839\n",
            "Total Timesteps: 353585 Episode Num: 402 Reward: 806.6065756249282\n",
            "Total Timesteps: 354585 Episode Num: 403 Reward: 472.6319924508872\n",
            "Total Timesteps: 355585 Episode Num: 404 Reward: 925.0889605376796\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 397.323332\n",
            "---------------------------------------\n",
            "Total Timesteps: 356585 Episode Num: 405 Reward: 370.8415548160424\n",
            "Total Timesteps: 357585 Episode Num: 406 Reward: 684.1525694073435\n",
            "Total Timesteps: 358585 Episode Num: 407 Reward: 555.2720411198907\n",
            "Total Timesteps: 359585 Episode Num: 408 Reward: 645.2873367825516\n",
            "Total Timesteps: 360585 Episode Num: 409 Reward: 881.5039358863306\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 758.815969\n",
            "---------------------------------------\n",
            "Total Timesteps: 361585 Episode Num: 410 Reward: 710.7400059128676\n",
            "Total Timesteps: 362585 Episode Num: 411 Reward: 672.321119219008\n",
            "Total Timesteps: 363585 Episode Num: 412 Reward: 695.6980651023578\n",
            "Total Timesteps: 364585 Episode Num: 413 Reward: 783.9093401314165\n",
            "Total Timesteps: 365585 Episode Num: 414 Reward: 420.9097752779142\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 878.068824\n",
            "---------------------------------------\n",
            "Total Timesteps: 366585 Episode Num: 415 Reward: 701.8332700818996\n",
            "Total Timesteps: 367585 Episode Num: 416 Reward: 863.1809213940055\n",
            "Total Timesteps: 368585 Episode Num: 417 Reward: 365.34077019364054\n",
            "Total Timesteps: 369585 Episode Num: 418 Reward: 721.4948505081695\n",
            "Total Timesteps: 370585 Episode Num: 419 Reward: 775.753052901758\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 735.093984\n",
            "---------------------------------------\n",
            "Total Timesteps: 371585 Episode Num: 420 Reward: 836.2988926048844\n",
            "Total Timesteps: 372585 Episode Num: 421 Reward: 813.0312391472827\n",
            "Total Timesteps: 373585 Episode Num: 422 Reward: 1022.9449463679126\n",
            "Total Timesteps: 374585 Episode Num: 423 Reward: 902.2004936754336\n",
            "Total Timesteps: 375585 Episode Num: 424 Reward: 686.9691375805511\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 793.997011\n",
            "---------------------------------------\n",
            "Total Timesteps: 376585 Episode Num: 425 Reward: 684.0180293733675\n",
            "Total Timesteps: 377585 Episode Num: 426 Reward: 1059.1523523841306\n",
            "Total Timesteps: 378585 Episode Num: 427 Reward: 1273.2910140998324\n",
            "Total Timesteps: 379585 Episode Num: 428 Reward: 1331.774465116154\n",
            "Total Timesteps: 380585 Episode Num: 429 Reward: 954.9701866147512\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1285.158392\n",
            "---------------------------------------\n",
            "Total Timesteps: 381585 Episode Num: 430 Reward: 1238.4689267924318\n",
            "Total Timesteps: 382585 Episode Num: 431 Reward: 1217.9231073231654\n",
            "Total Timesteps: 383585 Episode Num: 432 Reward: 1146.078777514112\n",
            "Total Timesteps: 384585 Episode Num: 433 Reward: 982.0903323817892\n",
            "Total Timesteps: 385585 Episode Num: 434 Reward: 925.0848225904408\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1053.274796\n",
            "---------------------------------------\n",
            "Total Timesteps: 386585 Episode Num: 435 Reward: 1115.4991995839769\n",
            "Total Timesteps: 387585 Episode Num: 436 Reward: 1320.9029371752845\n",
            "Total Timesteps: 388585 Episode Num: 437 Reward: 1297.4352323509236\n",
            "Total Timesteps: 389585 Episode Num: 438 Reward: 1398.4039421452198\n",
            "Total Timesteps: 390585 Episode Num: 439 Reward: 1221.639924919135\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1283.202035\n",
            "---------------------------------------\n",
            "Total Timesteps: 391585 Episode Num: 440 Reward: 1333.6134914312925\n",
            "Total Timesteps: 392585 Episode Num: 441 Reward: 1272.49146755514\n",
            "Total Timesteps: 393585 Episode Num: 442 Reward: 642.2359391638026\n",
            "Total Timesteps: 394585 Episode Num: 443 Reward: 1044.8561353898863\n",
            "Total Timesteps: 395585 Episode Num: 444 Reward: 1082.0954479490626\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 757.267696\n",
            "---------------------------------------\n",
            "Total Timesteps: 396585 Episode Num: 445 Reward: 988.8443502400684\n",
            "Total Timesteps: 397585 Episode Num: 446 Reward: 958.0845422771592\n",
            "Total Timesteps: 398585 Episode Num: 447 Reward: 682.9582924741504\n",
            "Total Timesteps: 399585 Episode Num: 448 Reward: 808.6471199477493\n",
            "Total Timesteps: 400585 Episode Num: 449 Reward: 866.8175502729723\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 886.980140\n",
            "---------------------------------------\n",
            "Total Timesteps: 401585 Episode Num: 450 Reward: 810.599839484267\n",
            "Total Timesteps: 402585 Episode Num: 451 Reward: 957.5947146014686\n",
            "Total Timesteps: 403585 Episode Num: 452 Reward: 823.366182473498\n",
            "Total Timesteps: 404585 Episode Num: 453 Reward: 979.4268839953567\n",
            "Total Timesteps: 405585 Episode Num: 454 Reward: 1117.683106207055\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 976.892077\n",
            "---------------------------------------\n",
            "Total Timesteps: 406585 Episode Num: 455 Reward: 956.838099467228\n",
            "Total Timesteps: 407585 Episode Num: 456 Reward: 1077.8526894112774\n",
            "Total Timesteps: 408585 Episode Num: 457 Reward: 935.1684495446544\n",
            "Total Timesteps: 409585 Episode Num: 458 Reward: 1115.1430118100288\n",
            "Total Timesteps: 410585 Episode Num: 459 Reward: 932.6519442149679\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1083.282439\n",
            "---------------------------------------\n",
            "Total Timesteps: 411585 Episode Num: 460 Reward: 1098.395860197151\n",
            "Total Timesteps: 412585 Episode Num: 461 Reward: 1522.469328776035\n",
            "Total Timesteps: 413585 Episode Num: 462 Reward: 1393.6483078648048\n",
            "Total Timesteps: 414585 Episode Num: 463 Reward: 1353.9447242367496\n",
            "Total Timesteps: 415585 Episode Num: 464 Reward: 1476.0648055410609\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1426.420852\n",
            "---------------------------------------\n",
            "Total Timesteps: 416585 Episode Num: 465 Reward: 1425.1717112164606\n",
            "Total Timesteps: 417585 Episode Num: 466 Reward: 1478.2479265655402\n",
            "Total Timesteps: 418585 Episode Num: 467 Reward: 1374.361646660642\n",
            "Total Timesteps: 419585 Episode Num: 468 Reward: 1469.2169250170546\n",
            "Total Timesteps: 420585 Episode Num: 469 Reward: 1396.892828832074\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1435.214812\n",
            "---------------------------------------\n",
            "Total Timesteps: 421585 Episode Num: 470 Reward: 1440.3320775162542\n",
            "Total Timesteps: 422585 Episode Num: 471 Reward: 1616.2618817017851\n",
            "Total Timesteps: 423585 Episode Num: 472 Reward: 1642.0432074057376\n",
            "Total Timesteps: 424585 Episode Num: 473 Reward: 1553.087072055144\n",
            "Total Timesteps: 425585 Episode Num: 474 Reward: 1629.7139804818794\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1551.207212\n",
            "---------------------------------------\n",
            "Total Timesteps: 426585 Episode Num: 475 Reward: 1534.0294091785609\n",
            "Total Timesteps: 427585 Episode Num: 476 Reward: 1444.1040672283893\n",
            "Total Timesteps: 428585 Episode Num: 477 Reward: 1456.626875380035\n",
            "Total Timesteps: 429585 Episode Num: 478 Reward: 1560.2296847190344\n",
            "Total Timesteps: 430585 Episode Num: 479 Reward: 1482.7531193040518\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1413.344674\n",
            "---------------------------------------\n",
            "Total Timesteps: 431585 Episode Num: 480 Reward: 1477.4430016237611\n",
            "Total Timesteps: 432585 Episode Num: 481 Reward: 1479.1671245192529\n",
            "Total Timesteps: 433585 Episode Num: 482 Reward: 1526.0815120324974\n",
            "Total Timesteps: 434585 Episode Num: 483 Reward: 1534.1605203329314\n",
            "Total Timesteps: 435585 Episode Num: 484 Reward: 1522.4119943288101\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1570.340956\n",
            "---------------------------------------\n",
            "Total Timesteps: 436585 Episode Num: 485 Reward: 1540.6411267673302\n",
            "Total Timesteps: 437585 Episode Num: 486 Reward: 1526.9091656041292\n",
            "Total Timesteps: 438585 Episode Num: 487 Reward: 1623.5604625399153\n",
            "Total Timesteps: 439585 Episode Num: 488 Reward: 1629.1570354649432\n",
            "Total Timesteps: 440585 Episode Num: 489 Reward: 1603.0458547666383\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1658.300576\n",
            "---------------------------------------\n",
            "Total Timesteps: 441585 Episode Num: 490 Reward: 1594.6225555458125\n",
            "Total Timesteps: 442585 Episode Num: 491 Reward: 1683.5718521517483\n",
            "Total Timesteps: 443585 Episode Num: 492 Reward: 1659.941795676862\n",
            "Total Timesteps: 444585 Episode Num: 493 Reward: 1704.5190045765257\n",
            "Total Timesteps: 445585 Episode Num: 494 Reward: 1710.8535796753304\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1486.366961\n",
            "---------------------------------------\n",
            "Total Timesteps: 446585 Episode Num: 495 Reward: 1549.3336435277477\n",
            "Total Timesteps: 447585 Episode Num: 496 Reward: 1850.1121554637216\n",
            "Total Timesteps: 448585 Episode Num: 497 Reward: 1712.8592131241182\n",
            "Total Timesteps: 449585 Episode Num: 498 Reward: 1758.025547273231\n",
            "Total Timesteps: 450585 Episode Num: 499 Reward: 1771.1761657685374\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1689.058924\n",
            "---------------------------------------\n",
            "Total Timesteps: 451585 Episode Num: 500 Reward: 1653.2046699344785\n",
            "Total Timesteps: 452585 Episode Num: 501 Reward: 1750.9615839892601\n",
            "Total Timesteps: 453585 Episode Num: 502 Reward: 1754.1850075980465\n",
            "Total Timesteps: 454585 Episode Num: 503 Reward: 1751.9267640929877\n",
            "Total Timesteps: 455585 Episode Num: 504 Reward: 1665.7486353596153\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1680.629939\n",
            "---------------------------------------\n",
            "Total Timesteps: 456585 Episode Num: 505 Reward: 1653.445795470175\n",
            "Total Timesteps: 457585 Episode Num: 506 Reward: 1851.1241754961914\n",
            "Total Timesteps: 458585 Episode Num: 507 Reward: 1726.776496941776\n",
            "Total Timesteps: 459585 Episode Num: 508 Reward: 1740.0316725808375\n",
            "Total Timesteps: 460585 Episode Num: 509 Reward: 1657.1972879438506\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1810.643612\n",
            "---------------------------------------\n",
            "Total Timesteps: 461585 Episode Num: 510 Reward: 1780.3035871833085\n",
            "Total Timesteps: 462585 Episode Num: 511 Reward: 1824.592900733413\n",
            "Total Timesteps: 463585 Episode Num: 512 Reward: 1804.3932052912198\n",
            "Total Timesteps: 464585 Episode Num: 513 Reward: 1677.7833289916414\n",
            "Total Timesteps: 465585 Episode Num: 514 Reward: 1752.8256953529956\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1834.859042\n",
            "---------------------------------------\n",
            "Total Timesteps: 466585 Episode Num: 515 Reward: 1804.6089155396025\n",
            "Total Timesteps: 467585 Episode Num: 516 Reward: 1860.267052522488\n",
            "Total Timesteps: 468585 Episode Num: 517 Reward: 1779.668846291663\n",
            "Total Timesteps: 469585 Episode Num: 518 Reward: 1801.8806959570002\n",
            "Total Timesteps: 470585 Episode Num: 519 Reward: 1842.0116169011137\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1797.952994\n",
            "---------------------------------------\n",
            "Total Timesteps: 471585 Episode Num: 520 Reward: 1849.1148500555312\n",
            "Total Timesteps: 472585 Episode Num: 521 Reward: 1859.2977487496273\n",
            "Total Timesteps: 473585 Episode Num: 522 Reward: 1803.374664742668\n",
            "Total Timesteps: 474585 Episode Num: 523 Reward: 1801.9534163540104\n",
            "Total Timesteps: 475585 Episode Num: 524 Reward: 2045.2532058690265\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1869.482231\n",
            "---------------------------------------\n",
            "Total Timesteps: 476585 Episode Num: 525 Reward: 1829.036721581928\n",
            "Total Timesteps: 477585 Episode Num: 526 Reward: 1797.5193931590165\n",
            "Total Timesteps: 478585 Episode Num: 527 Reward: 1917.057570074558\n",
            "Total Timesteps: 479585 Episode Num: 528 Reward: 1837.6730517367168\n",
            "Total Timesteps: 480585 Episode Num: 529 Reward: 1876.8056648031277\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1838.264324\n",
            "---------------------------------------\n",
            "Total Timesteps: 481585 Episode Num: 530 Reward: 1798.6044450367292\n",
            "Total Timesteps: 482585 Episode Num: 531 Reward: 2030.0598138156327\n",
            "Total Timesteps: 483585 Episode Num: 532 Reward: 1992.4085671832183\n",
            "Total Timesteps: 484585 Episode Num: 533 Reward: 2000.8965829397887\n",
            "Total Timesteps: 485585 Episode Num: 534 Reward: 1947.8232663893086\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1927.069609\n",
            "---------------------------------------\n",
            "Total Timesteps: 486585 Episode Num: 535 Reward: 1899.8048511616144\n",
            "Total Timesteps: 487585 Episode Num: 536 Reward: 1843.271140522842\n",
            "Total Timesteps: 488585 Episode Num: 537 Reward: 1944.626706098717\n",
            "Total Timesteps: 489585 Episode Num: 538 Reward: 1917.7653060062592\n",
            "Total Timesteps: 490585 Episode Num: 539 Reward: 1884.0923234975003\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2039.831961\n",
            "---------------------------------------\n",
            "Total Timesteps: 491585 Episode Num: 540 Reward: 1987.3598922296821\n",
            "Total Timesteps: 492585 Episode Num: 541 Reward: 1948.7307377322945\n",
            "Total Timesteps: 493585 Episode Num: 542 Reward: 1966.1515134636445\n",
            "Total Timesteps: 494585 Episode Num: 543 Reward: 1899.9124280679368\n",
            "Total Timesteps: 495585 Episode Num: 544 Reward: 1967.5437950273754\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2012.158282\n",
            "---------------------------------------\n",
            "Total Timesteps: 496585 Episode Num: 545 Reward: 1984.8119990415294\n",
            "Total Timesteps: 497585 Episode Num: 546 Reward: 1888.1900242691947\n",
            "Total Timesteps: 498585 Episode Num: 547 Reward: 2046.0079441975215\n",
            "Total Timesteps: 499585 Episode Num: 548 Reward: 1861.6091853414032\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1978.351616\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "67fd06f8-b3e1-41d4-da9e-7d4d45aa2e71"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1982.945658\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcSNoQj3b7eI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "1b3ae78e-f911-4c14-ef8f-6db836aefba9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypdIkehEb9ti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d5e0f07-a0bf-4c88-c5c7-e93453a6a19b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  exp  pytorch_models  results  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qM9CsN2cMEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir Reiforcement_Outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZLk16olch77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d21ad50-79ab-41ad-846a-4043afc2042b"
      },
      "source": [
        "cd Reiforcement_Outputs"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Reiforcement_Outputs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVEv1QMBckq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/exp/brs/monitor/openaigym.video.0.1591.video000001.mp4 /content/drive/\"My Drive\"/Reiforcement_Outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_omL6SadJGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/pytorch_models/TD3_AntBulletEnv-v0_0_actor.pth /content/drive/\"My Drive\"/Reiforcement_Outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQJu_bd3dMa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/pytorch_models/TD3_AntBulletEnv-v0_0_critic.pth /content/drive/\"My Drive\"/Reiforcement_Outputs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}