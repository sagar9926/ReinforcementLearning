{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_HalfCheetahBulletEnv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagar9926/ReinforcementLearning/blob/master/TD3_HalfCheetahBulletEnv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxW2eNW5LotA",
        "colab_type": "code",
        "outputId": "f80cceb1-ff49-4109-bc41-0bdea4bc065f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "!pip3 install pybullet --upgrade\n",
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pybullet in /usr/local/lib/python3.6/dist-packages (2.7.9)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Initializing the Experience Replay memory\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object): # since not inherriting from any other class thus passing object\n",
        "\n",
        "  def __init__(self, max_size=1000000): \n",
        "    self.storage = []                             # This is memory itself\n",
        "    self.max_size = max_size                      \n",
        "    self.ptr = 0                                  # index of different cells of memory, use to add transition to memory or do some sampling\n",
        "\n",
        "  def add(self, transition):                      # adds any new transition to the memory\n",
        "    if len(self.storage) == self.max_size:        # Check if memory has been fully populated\n",
        "      self.storage[int(self.ptr)] = transition    # if yes then the new transition should be added at the beginning\n",
        "      self.ptr = (self.ptr + 1) % self.max_size   #  Incrementing the pointer\n",
        "    else:\n",
        "      self.storage.append(transition)            # if memory is not fully populated , then just append the new transition \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def sample(self, batch_size):                 # Sample the transitions from memory and put them in the batch\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size) # generating some random indexes equal to batch size\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], [] \n",
        "    for i in ind: # iterate over the random indexes \n",
        "\n",
        "      state, next_state, action, reward, done = self.storage[i] # get the transition present at index i\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "      # before we convert batches into torch tensors we have to convert all of them to numpy array, Reshape converts the array into 1d array\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    #max action is to clip in case we added too much noise \n",
        "    super(Actor, self).__init__() # Activate the inherritance\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x)) #applying RELU breaks the non linearity\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) # Tanh returns the value between -1 and 1.By multiplying by max action we get continuous \n",
        "                                                      # values for actions within the permissible range \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # since we are going to use two critic model at the same time thus as defined below\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1) #output is one as we output single Q value\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1) # Concatenation of State and Action Vertically\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u): # this will be used for the step of gradient ascent of actor model. \n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object): # since not inherriting from any other class thus passing object as parameter\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action): # In this method we will create all our objects models and optimizers\n",
        "\n",
        "    #making sure our T3D class can work with any environment\n",
        "    # Create all our steps on device selected\n",
        "\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)  # Trained via GradientAscent\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device) # Trained vis Polyak Averaging\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict()) #load the actor target with the weights of the actor model\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters()) \n",
        "\n",
        "\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "\n",
        "\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device) #Converting to torch tensor 1D array\n",
        "    return self.actor(state).cpu().data.numpy().flatten() # Feeding the state to actor model to get the action\n",
        "    # Force the computation of forward pass on CPU and extract the data and convert to numpy and flatten\n",
        "    # the output to get 1D array\n",
        "\n",
        "    # Why numpy -> Torch -> Numpy\n",
        "    # At some point we need to use neural network for which we need pytorch format but we convert it back to numpy coz at some time we clip\n",
        "    # the action after adding noise to it and we are adding noise and clipping by numpy\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      # since we need to fed them to Neural Network thus convert them to Torch tensor\n",
        "      state = torch.Tensor(batch_states).to(device) # Will go in NN # This is still a batch of states\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "\n",
        "      #Generate the Gaussian noise for each of the elements of the batch of next actions\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)  # we create a noise tensor of size same size as batch actons \n",
        "      # Clipping the gaussian noise\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      # Adding noise and clippind the action \n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      # we are training the model for the entire episode\n",
        "      # min(Qt1, Qt2) this represents the value of the next state\n",
        "      # and if we are at the end of episode there is no next state and we have to start new episode\n",
        "      # therefore min(Qt1, Qt2) this doesnt apply any more\n",
        "      \n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0: # Delayed part of T3 DDPG model\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean() # Negative sign because we are performing gradient Ascend\n",
        "        # In above step we don't input here the current action played corresponding to state\n",
        "        # Differentiate wrt the actor parameters\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward() # Gradient Ascend step\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"HalfCheetahBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 1e6 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "f1fb9027-27bf-4923-9a56-abaf7a73a650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "14f9273e-9bad-43bb-a429-7ed106c264b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "f40f107c-0b72-46e1-b6db-a67c59b3e290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1429.426662\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "baf2617f-5b95-4a3b-fe72-4f07a34e1408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  #print(\"total_timesteps:\",total_timesteps)\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "      #print(\"episode_timesteps:\",episode_timesteps)\n",
        "\n",
        "    #print(\"timesteps_since_eval:\",timesteps_since_eval)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: -1170.1538310200217\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: -1327.648270005437\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: -1254.250432272112\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: -1204.5703885819726\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: -1366.4041825072854\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1432.707332\n",
            "---------------------------------------\n",
            "Total Timesteps: 6000 Episode Num: 6 Reward: -1248.0280482201886\n",
            "Total Timesteps: 7000 Episode Num: 7 Reward: -1074.6927529412187\n",
            "Total Timesteps: 8000 Episode Num: 8 Reward: -1470.095925158508\n",
            "Total Timesteps: 9000 Episode Num: 9 Reward: -1174.7657755896453\n",
            "Total Timesteps: 10000 Episode Num: 10 Reward: -1147.5243445742399\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1477.463945\n",
            "---------------------------------------\n",
            "Total Timesteps: 11000 Episode Num: 11 Reward: -1494.24407154186\n",
            "Total Timesteps: 12000 Episode Num: 12 Reward: -1326.6749250802723\n",
            "Total Timesteps: 13000 Episode Num: 13 Reward: -1438.087429162818\n",
            "Total Timesteps: 14000 Episode Num: 14 Reward: -1687.169248097188\n",
            "Total Timesteps: 15000 Episode Num: 15 Reward: -1708.43995250817\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1256.092380\n",
            "---------------------------------------\n",
            "Total Timesteps: 16000 Episode Num: 16 Reward: -927.2904385940477\n",
            "Total Timesteps: 17000 Episode Num: 17 Reward: -1693.8552007858361\n",
            "Total Timesteps: 18000 Episode Num: 18 Reward: 279.3398756162489\n",
            "Total Timesteps: 19000 Episode Num: 19 Reward: -1318.469938295558\n",
            "Total Timesteps: 20000 Episode Num: 20 Reward: 647.3251764053487\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -987.645006\n",
            "---------------------------------------\n",
            "Total Timesteps: 21000 Episode Num: 21 Reward: -1145.3531016677894\n",
            "Total Timesteps: 22000 Episode Num: 22 Reward: -824.0911830559198\n",
            "Total Timesteps: 23000 Episode Num: 23 Reward: -1574.5042757223544\n",
            "Total Timesteps: 24000 Episode Num: 24 Reward: -1401.5144670290424\n",
            "Total Timesteps: 25000 Episode Num: 25 Reward: -60.883147759770836\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -566.875348\n",
            "---------------------------------------\n",
            "Total Timesteps: 26000 Episode Num: 26 Reward: -1306.4368284818772\n",
            "Total Timesteps: 27000 Episode Num: 27 Reward: -1255.457634204925\n",
            "Total Timesteps: 28000 Episode Num: 28 Reward: -1234.2593260314188\n",
            "Total Timesteps: 29000 Episode Num: 29 Reward: -1191.103437591621\n",
            "Total Timesteps: 30000 Episode Num: 30 Reward: -754.2402693117868\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -801.358587\n",
            "---------------------------------------\n",
            "Total Timesteps: 31000 Episode Num: 31 Reward: -626.2544820268647\n",
            "Total Timesteps: 32000 Episode Num: 32 Reward: 607.2349528099303\n",
            "Total Timesteps: 33000 Episode Num: 33 Reward: 296.5381399270427\n",
            "Total Timesteps: 34000 Episode Num: 34 Reward: -633.0703343709326\n",
            "Total Timesteps: 35000 Episode Num: 35 Reward: -1112.0766093597397\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 329.923144\n",
            "---------------------------------------\n",
            "Total Timesteps: 36000 Episode Num: 36 Reward: 454.16540334406216\n",
            "Total Timesteps: 37000 Episode Num: 37 Reward: -926.3223664791375\n",
            "Total Timesteps: 38000 Episode Num: 38 Reward: -360.14531899838533\n",
            "Total Timesteps: 39000 Episode Num: 39 Reward: -985.0118250787013\n",
            "Total Timesteps: 40000 Episode Num: 40 Reward: 157.78004310750615\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 429.193758\n",
            "---------------------------------------\n",
            "Total Timesteps: 41000 Episode Num: 41 Reward: 103.10872115137552\n",
            "Total Timesteps: 42000 Episode Num: 42 Reward: 588.9029777333674\n",
            "Total Timesteps: 43000 Episode Num: 43 Reward: -1085.737447378325\n",
            "Total Timesteps: 44000 Episode Num: 44 Reward: 326.71984405220627\n",
            "Total Timesteps: 45000 Episode Num: 45 Reward: 527.1354210600477\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 577.178458\n",
            "---------------------------------------\n",
            "Total Timesteps: 46000 Episode Num: 46 Reward: 627.8930678286505\n",
            "Total Timesteps: 47000 Episode Num: 47 Reward: 630.9941492115786\n",
            "Total Timesteps: 48000 Episode Num: 48 Reward: 553.6261202906967\n",
            "Total Timesteps: 49000 Episode Num: 49 Reward: 620.6904505753002\n",
            "Total Timesteps: 50000 Episode Num: 50 Reward: 294.68321560071337\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 427.259131\n",
            "---------------------------------------\n",
            "Total Timesteps: 51000 Episode Num: 51 Reward: 462.61201066635067\n",
            "Total Timesteps: 52000 Episode Num: 52 Reward: 114.17315562767443\n",
            "Total Timesteps: 53000 Episode Num: 53 Reward: 510.52201976198796\n",
            "Total Timesteps: 54000 Episode Num: 54 Reward: 592.714028483104\n",
            "Total Timesteps: 55000 Episode Num: 55 Reward: 448.2976207481855\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 627.412952\n",
            "---------------------------------------\n",
            "Total Timesteps: 56000 Episode Num: 56 Reward: 555.0464732704075\n",
            "Total Timesteps: 57000 Episode Num: 57 Reward: 659.0717784049608\n",
            "Total Timesteps: 58000 Episode Num: 58 Reward: 455.67665975956146\n",
            "Total Timesteps: 59000 Episode Num: 59 Reward: 451.99741888824377\n",
            "Total Timesteps: 60000 Episode Num: 60 Reward: 441.85529838852057\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 680.606308\n",
            "---------------------------------------\n",
            "Total Timesteps: 61000 Episode Num: 61 Reward: 596.600507531182\n",
            "Total Timesteps: 62000 Episode Num: 62 Reward: 633.0248362049127\n",
            "Total Timesteps: 63000 Episode Num: 63 Reward: 356.8994549411332\n",
            "Total Timesteps: 64000 Episode Num: 64 Reward: 514.3813488614043\n",
            "Total Timesteps: 65000 Episode Num: 65 Reward: 659.2439050659736\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 592.910079\n",
            "---------------------------------------\n",
            "Total Timesteps: 66000 Episode Num: 66 Reward: 745.2444937653461\n",
            "Total Timesteps: 67000 Episode Num: 67 Reward: 755.8952666940341\n",
            "Total Timesteps: 68000 Episode Num: 68 Reward: 763.5410479574646\n",
            "Total Timesteps: 69000 Episode Num: 69 Reward: 817.6283125802704\n",
            "Total Timesteps: 70000 Episode Num: 70 Reward: 360.4529538251016\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 264.410961\n",
            "---------------------------------------\n",
            "Total Timesteps: 71000 Episode Num: 71 Reward: 758.0525746258804\n",
            "Total Timesteps: 72000 Episode Num: 72 Reward: 761.0423339952295\n",
            "Total Timesteps: 73000 Episode Num: 73 Reward: 674.5761541572074\n",
            "Total Timesteps: 74000 Episode Num: 74 Reward: 670.6604240425893\n",
            "Total Timesteps: 75000 Episode Num: 75 Reward: 676.5253109106818\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 689.794208\n",
            "---------------------------------------\n",
            "Total Timesteps: 76000 Episode Num: 76 Reward: 679.6257733870933\n",
            "Total Timesteps: 77000 Episode Num: 77 Reward: 783.8010107945435\n",
            "Total Timesteps: 78000 Episode Num: 78 Reward: 950.467081427333\n",
            "Total Timesteps: 79000 Episode Num: 79 Reward: 566.6698850340615\n",
            "Total Timesteps: 80000 Episode Num: 80 Reward: 580.9592299545744\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 745.614913\n",
            "---------------------------------------\n",
            "Total Timesteps: 81000 Episode Num: 81 Reward: 503.0174329850184\n",
            "Total Timesteps: 82000 Episode Num: 82 Reward: 566.2658220353657\n",
            "Total Timesteps: 83000 Episode Num: 83 Reward: 599.7248056080836\n",
            "Total Timesteps: 84000 Episode Num: 84 Reward: 651.1577896936872\n",
            "Total Timesteps: 85000 Episode Num: 85 Reward: 696.4684610971032\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 575.091173\n",
            "---------------------------------------\n",
            "Total Timesteps: 86000 Episode Num: 86 Reward: 433.88820508990216\n",
            "Total Timesteps: 87000 Episode Num: 87 Reward: 614.2300321047393\n",
            "Total Timesteps: 88000 Episode Num: 88 Reward: 941.8755908658774\n",
            "Total Timesteps: 89000 Episode Num: 89 Reward: 745.9659433202306\n",
            "Total Timesteps: 90000 Episode Num: 90 Reward: 832.6358510321155\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 584.550661\n",
            "---------------------------------------\n",
            "Total Timesteps: 91000 Episode Num: 91 Reward: 679.0570521720504\n",
            "Total Timesteps: 92000 Episode Num: 92 Reward: 674.835835940561\n",
            "Total Timesteps: 93000 Episode Num: 93 Reward: 525.1351972560863\n",
            "Total Timesteps: 94000 Episode Num: 94 Reward: 805.7719934753512\n",
            "Total Timesteps: 95000 Episode Num: 95 Reward: 685.6973254939255\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 721.010872\n",
            "---------------------------------------\n",
            "Total Timesteps: 96000 Episode Num: 96 Reward: 695.5282915010698\n",
            "Total Timesteps: 97000 Episode Num: 97 Reward: 864.0462053710263\n",
            "Total Timesteps: 98000 Episode Num: 98 Reward: 1022.8889269265085\n",
            "Total Timesteps: 99000 Episode Num: 99 Reward: 746.4612922693733\n",
            "Total Timesteps: 100000 Episode Num: 100 Reward: 911.7389230786563\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 990.680607\n",
            "---------------------------------------\n",
            "Total Timesteps: 101000 Episode Num: 101 Reward: 988.2633144325954\n",
            "Total Timesteps: 102000 Episode Num: 102 Reward: 1097.382252781097\n",
            "Total Timesteps: 103000 Episode Num: 103 Reward: 1066.4579084277254\n",
            "Total Timesteps: 104000 Episode Num: 104 Reward: 974.0237513948251\n",
            "Total Timesteps: 105000 Episode Num: 105 Reward: 1015.9295974684128\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1253.172928\n",
            "---------------------------------------\n",
            "Total Timesteps: 106000 Episode Num: 106 Reward: 1211.8969870887458\n",
            "Total Timesteps: 107000 Episode Num: 107 Reward: 1281.0208014711982\n",
            "Total Timesteps: 108000 Episode Num: 108 Reward: 1206.4459134086537\n",
            "Total Timesteps: 109000 Episode Num: 109 Reward: 1169.0548703619052\n",
            "Total Timesteps: 110000 Episode Num: 110 Reward: 983.1239933612342\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1173.318805\n",
            "---------------------------------------\n",
            "Total Timesteps: 111000 Episode Num: 111 Reward: 1219.0048546274056\n",
            "Total Timesteps: 112000 Episode Num: 112 Reward: 755.8626058840895\n",
            "Total Timesteps: 113000 Episode Num: 113 Reward: 1206.5397832382253\n",
            "Total Timesteps: 114000 Episode Num: 114 Reward: 724.1280252361242\n",
            "Total Timesteps: 115000 Episode Num: 115 Reward: 1345.1784807059696\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1428.200098\n",
            "---------------------------------------\n",
            "Total Timesteps: 116000 Episode Num: 116 Reward: 1455.5717244808343\n",
            "Total Timesteps: 117000 Episode Num: 117 Reward: 1197.0281282497428\n",
            "Total Timesteps: 118000 Episode Num: 118 Reward: 1125.3874568315186\n",
            "Total Timesteps: 119000 Episode Num: 119 Reward: 1423.9390118663848\n",
            "Total Timesteps: 120000 Episode Num: 120 Reward: 1128.0929932071226\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1347.039334\n",
            "---------------------------------------\n",
            "Total Timesteps: 121000 Episode Num: 121 Reward: 1101.8239641799953\n",
            "Total Timesteps: 122000 Episode Num: 122 Reward: 1553.7125815559186\n",
            "Total Timesteps: 123000 Episode Num: 123 Reward: 1300.5995732500683\n",
            "Total Timesteps: 124000 Episode Num: 124 Reward: 1226.6623236843504\n",
            "Total Timesteps: 125000 Episode Num: 125 Reward: 1444.319559188189\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1420.626284\n",
            "---------------------------------------\n",
            "Total Timesteps: 126000 Episode Num: 126 Reward: 1415.3606626258636\n",
            "Total Timesteps: 127000 Episode Num: 127 Reward: 1409.336574329895\n",
            "Total Timesteps: 128000 Episode Num: 128 Reward: 1460.5777316223016\n",
            "Total Timesteps: 129000 Episode Num: 129 Reward: 1604.2126111345285\n",
            "Total Timesteps: 130000 Episode Num: 130 Reward: 1529.7251205953194\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1518.102231\n",
            "---------------------------------------\n",
            "Total Timesteps: 131000 Episode Num: 131 Reward: 1488.973081230899\n",
            "Total Timesteps: 132000 Episode Num: 132 Reward: 1541.4812317440676\n",
            "Total Timesteps: 133000 Episode Num: 133 Reward: 1431.6934112877611\n",
            "Total Timesteps: 134000 Episode Num: 134 Reward: 714.8005545767717\n",
            "Total Timesteps: 135000 Episode Num: 135 Reward: 404.8006173749843\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 913.571341\n",
            "---------------------------------------\n",
            "Total Timesteps: 136000 Episode Num: 136 Reward: 762.0787049337667\n",
            "Total Timesteps: 137000 Episode Num: 137 Reward: 771.4328849270432\n",
            "Total Timesteps: 138000 Episode Num: 138 Reward: 1079.4805215272722\n",
            "Total Timesteps: 139000 Episode Num: 139 Reward: 655.8784448250901\n",
            "Total Timesteps: 140000 Episode Num: 140 Reward: 591.4973854713405\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1284.508775\n",
            "---------------------------------------\n",
            "Total Timesteps: 141000 Episode Num: 141 Reward: 702.743537437029\n",
            "Total Timesteps: 142000 Episode Num: 142 Reward: 1447.4213867185522\n",
            "Total Timesteps: 143000 Episode Num: 143 Reward: 748.9508630525972\n",
            "Total Timesteps: 144000 Episode Num: 144 Reward: 998.4965682342968\n",
            "Total Timesteps: 145000 Episode Num: 145 Reward: 1609.853007211232\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 780.690117\n",
            "---------------------------------------\n",
            "Total Timesteps: 146000 Episode Num: 146 Reward: 611.8159940127244\n",
            "Total Timesteps: 147000 Episode Num: 147 Reward: 655.1732726857176\n",
            "Total Timesteps: 148000 Episode Num: 148 Reward: 1355.4796965690514\n",
            "Total Timesteps: 149000 Episode Num: 149 Reward: 1526.6994511479697\n",
            "Total Timesteps: 150000 Episode Num: 150 Reward: 1524.8385201983242\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1479.269350\n",
            "---------------------------------------\n",
            "Total Timesteps: 151000 Episode Num: 151 Reward: 1477.51345896182\n",
            "Total Timesteps: 152000 Episode Num: 152 Reward: 1525.9420308180815\n",
            "Total Timesteps: 153000 Episode Num: 153 Reward: 1443.0130991406425\n",
            "Total Timesteps: 154000 Episode Num: 154 Reward: 1419.5388133215547\n",
            "Total Timesteps: 155000 Episode Num: 155 Reward: 1586.9566167189812\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1613.863672\n",
            "---------------------------------------\n",
            "Total Timesteps: 156000 Episode Num: 156 Reward: 1582.4870076466057\n",
            "Total Timesteps: 157000 Episode Num: 157 Reward: 1599.1204049292242\n",
            "Total Timesteps: 158000 Episode Num: 158 Reward: 1492.5625698396118\n",
            "Total Timesteps: 159000 Episode Num: 159 Reward: 1160.3239727023101\n",
            "Total Timesteps: 160000 Episode Num: 160 Reward: 1688.1617464556484\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1729.904806\n",
            "---------------------------------------\n",
            "Total Timesteps: 161000 Episode Num: 161 Reward: 1720.703390458316\n",
            "Total Timesteps: 162000 Episode Num: 162 Reward: 1625.5146521611578\n",
            "Total Timesteps: 163000 Episode Num: 163 Reward: 1692.301669206301\n",
            "Total Timesteps: 164000 Episode Num: 164 Reward: 1665.0185223428086\n",
            "Total Timesteps: 165000 Episode Num: 165 Reward: 514.8299249772605\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1742.798542\n",
            "---------------------------------------\n",
            "Total Timesteps: 166000 Episode Num: 166 Reward: 1683.9920828158226\n",
            "Total Timesteps: 167000 Episode Num: 167 Reward: 1681.096200578539\n",
            "Total Timesteps: 168000 Episode Num: 168 Reward: 1750.7938797548475\n",
            "Total Timesteps: 169000 Episode Num: 169 Reward: 1630.6918533465348\n",
            "Total Timesteps: 170000 Episode Num: 170 Reward: 1613.1215123146155\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1525.108993\n",
            "---------------------------------------\n",
            "Total Timesteps: 171000 Episode Num: 171 Reward: 1494.8855130765721\n",
            "Total Timesteps: 172000 Episode Num: 172 Reward: 1651.189222513161\n",
            "Total Timesteps: 173000 Episode Num: 173 Reward: 1642.6724672885061\n",
            "Total Timesteps: 174000 Episode Num: 174 Reward: 1722.2144967729223\n",
            "Total Timesteps: 175000 Episode Num: 175 Reward: 1761.8719300508346\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1666.494051\n",
            "---------------------------------------\n",
            "Total Timesteps: 176000 Episode Num: 176 Reward: 1627.8304162220882\n",
            "Total Timesteps: 177000 Episode Num: 177 Reward: 1583.0007804633542\n",
            "Total Timesteps: 178000 Episode Num: 178 Reward: 1567.6214767902643\n",
            "Total Timesteps: 179000 Episode Num: 179 Reward: 1726.2804565299425\n",
            "Total Timesteps: 180000 Episode Num: 180 Reward: 1769.302216910122\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1763.013317\n",
            "---------------------------------------\n",
            "Total Timesteps: 181000 Episode Num: 181 Reward: 1700.4051336305197\n",
            "Total Timesteps: 182000 Episode Num: 182 Reward: 1808.004242828289\n",
            "Total Timesteps: 183000 Episode Num: 183 Reward: 1695.8080455771308\n",
            "Total Timesteps: 184000 Episode Num: 184 Reward: 1787.9107551262987\n",
            "Total Timesteps: 185000 Episode Num: 185 Reward: 1744.8742344339666\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1880.516989\n",
            "---------------------------------------\n",
            "Total Timesteps: 186000 Episode Num: 186 Reward: 1830.3925634639695\n",
            "Total Timesteps: 187000 Episode Num: 187 Reward: 1842.6528195904714\n",
            "Total Timesteps: 188000 Episode Num: 188 Reward: 1835.143433736855\n",
            "Total Timesteps: 189000 Episode Num: 189 Reward: 1846.6888672885352\n",
            "Total Timesteps: 190000 Episode Num: 190 Reward: 1623.7932556667015\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1781.718415\n",
            "---------------------------------------\n",
            "Total Timesteps: 191000 Episode Num: 191 Reward: 1721.8495864852557\n",
            "Total Timesteps: 192000 Episode Num: 192 Reward: 1813.0662041651542\n",
            "Total Timesteps: 193000 Episode Num: 193 Reward: 1803.1310024105023\n",
            "Total Timesteps: 194000 Episode Num: 194 Reward: 1766.6551835831956\n",
            "Total Timesteps: 195000 Episode Num: 195 Reward: 1749.3939361943576\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1804.289052\n",
            "---------------------------------------\n",
            "Total Timesteps: 196000 Episode Num: 196 Reward: 1782.3716851556928\n",
            "Total Timesteps: 197000 Episode Num: 197 Reward: 1475.6039052611238\n",
            "Total Timesteps: 198000 Episode Num: 198 Reward: 1788.6030658212644\n",
            "Total Timesteps: 199000 Episode Num: 199 Reward: 1725.3995548249043\n",
            "Total Timesteps: 200000 Episode Num: 200 Reward: 1737.2814908377336\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1780.576958\n",
            "---------------------------------------\n",
            "Total Timesteps: 201000 Episode Num: 201 Reward: 1707.23949456497\n",
            "Total Timesteps: 202000 Episode Num: 202 Reward: 1744.6389234711917\n",
            "Total Timesteps: 203000 Episode Num: 203 Reward: 1838.1404813506388\n",
            "Total Timesteps: 204000 Episode Num: 204 Reward: 1796.8499469761864\n",
            "Total Timesteps: 205000 Episode Num: 205 Reward: 1589.7439302244302\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1860.042336\n",
            "---------------------------------------\n",
            "Total Timesteps: 206000 Episode Num: 206 Reward: 1807.2198022075543\n",
            "Total Timesteps: 207000 Episode Num: 207 Reward: 1691.5864127233565\n",
            "Total Timesteps: 208000 Episode Num: 208 Reward: 1883.3820496984379\n",
            "Total Timesteps: 209000 Episode Num: 209 Reward: 1899.2425887633983\n",
            "Total Timesteps: 210000 Episode Num: 210 Reward: 1874.1968719030813\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1815.031164\n",
            "---------------------------------------\n",
            "Total Timesteps: 211000 Episode Num: 211 Reward: 1761.216922024091\n",
            "Total Timesteps: 212000 Episode Num: 212 Reward: 1859.6153820301474\n",
            "Total Timesteps: 213000 Episode Num: 213 Reward: 1774.5020421791407\n",
            "Total Timesteps: 214000 Episode Num: 214 Reward: 1771.2089513017327\n",
            "Total Timesteps: 215000 Episode Num: 215 Reward: 1837.0041958801487\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1530.676368\n",
            "---------------------------------------\n",
            "Total Timesteps: 216000 Episode Num: 216 Reward: 1811.425299039101\n",
            "Total Timesteps: 217000 Episode Num: 217 Reward: 1805.660700976268\n",
            "Total Timesteps: 218000 Episode Num: 218 Reward: 1880.8906671614996\n",
            "Total Timesteps: 219000 Episode Num: 219 Reward: 1444.3326216028968\n",
            "Total Timesteps: 220000 Episode Num: 220 Reward: 1890.9939514202888\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1868.708415\n",
            "---------------------------------------\n",
            "Total Timesteps: 221000 Episode Num: 221 Reward: 1855.3955529925433\n",
            "Total Timesteps: 222000 Episode Num: 222 Reward: 1836.046726224343\n",
            "Total Timesteps: 223000 Episode Num: 223 Reward: 1792.0269302378226\n",
            "Total Timesteps: 224000 Episode Num: 224 Reward: 1919.8397858028197\n",
            "Total Timesteps: 225000 Episode Num: 225 Reward: 1807.6655166951823\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1193.615573\n",
            "---------------------------------------\n",
            "Total Timesteps: 226000 Episode Num: 226 Reward: 1891.9835967733109\n",
            "Total Timesteps: 227000 Episode Num: 227 Reward: 1842.6933488741395\n",
            "Total Timesteps: 228000 Episode Num: 228 Reward: 1780.8131636213154\n",
            "Total Timesteps: 229000 Episode Num: 229 Reward: 1880.6377867689607\n",
            "Total Timesteps: 230000 Episode Num: 230 Reward: 1842.93198829823\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1604.749306\n",
            "---------------------------------------\n",
            "Total Timesteps: 231000 Episode Num: 231 Reward: 1792.6787530734205\n",
            "Total Timesteps: 232000 Episode Num: 232 Reward: 1718.699606290807\n",
            "Total Timesteps: 233000 Episode Num: 233 Reward: 1748.8523007695505\n",
            "Total Timesteps: 234000 Episode Num: 234 Reward: 1868.8421364138069\n",
            "Total Timesteps: 235000 Episode Num: 235 Reward: 1820.7082205012534\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1882.425238\n",
            "---------------------------------------\n",
            "Total Timesteps: 236000 Episode Num: 236 Reward: 1487.0254939291021\n",
            "Total Timesteps: 237000 Episode Num: 237 Reward: 1826.8865651605497\n",
            "Total Timesteps: 238000 Episode Num: 238 Reward: 671.3910709715757\n",
            "Total Timesteps: 239000 Episode Num: 239 Reward: 1821.4583363146528\n",
            "Total Timesteps: 240000 Episode Num: 240 Reward: 1850.2874256821501\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1909.077332\n",
            "---------------------------------------\n",
            "Total Timesteps: 241000 Episode Num: 241 Reward: 1878.8276001141362\n",
            "Total Timesteps: 242000 Episode Num: 242 Reward: 1904.2426877926064\n",
            "Total Timesteps: 243000 Episode Num: 243 Reward: 1837.9240081790574\n",
            "Total Timesteps: 244000 Episode Num: 244 Reward: 1821.3980438186452\n",
            "Total Timesteps: 245000 Episode Num: 245 Reward: 1835.5825526304905\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1895.849831\n",
            "---------------------------------------\n",
            "Total Timesteps: 246000 Episode Num: 246 Reward: 1876.8157299994584\n",
            "Total Timesteps: 247000 Episode Num: 247 Reward: 1887.7514761915475\n",
            "Total Timesteps: 248000 Episode Num: 248 Reward: 1882.6678860917552\n",
            "Total Timesteps: 249000 Episode Num: 249 Reward: 1991.8542873968956\n",
            "Total Timesteps: 250000 Episode Num: 250 Reward: 1886.4835090986965\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1877.682473\n",
            "---------------------------------------\n",
            "Total Timesteps: 251000 Episode Num: 251 Reward: 1825.9991551530707\n",
            "Total Timesteps: 252000 Episode Num: 252 Reward: 1821.8259064194383\n",
            "Total Timesteps: 253000 Episode Num: 253 Reward: 1855.6150632769168\n",
            "Total Timesteps: 254000 Episode Num: 254 Reward: 1787.1153591833101\n",
            "Total Timesteps: 255000 Episode Num: 255 Reward: 1732.201756704782\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1878.997846\n",
            "---------------------------------------\n",
            "Total Timesteps: 256000 Episode Num: 256 Reward: 1824.8448293251918\n",
            "Total Timesteps: 257000 Episode Num: 257 Reward: 1809.6535003446504\n",
            "Total Timesteps: 258000 Episode Num: 258 Reward: 1934.5078503170641\n",
            "Total Timesteps: 259000 Episode Num: 259 Reward: 1910.3482247088373\n",
            "Total Timesteps: 260000 Episode Num: 260 Reward: 1832.5046315294271\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1988.848491\n",
            "---------------------------------------\n",
            "Total Timesteps: 261000 Episode Num: 261 Reward: 1962.3909166588846\n",
            "Total Timesteps: 262000 Episode Num: 262 Reward: 1930.975098791455\n",
            "Total Timesteps: 263000 Episode Num: 263 Reward: 1906.387197336854\n",
            "Total Timesteps: 264000 Episode Num: 264 Reward: 1911.9553207800589\n",
            "Total Timesteps: 265000 Episode Num: 265 Reward: 1920.498059292347\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1955.467964\n",
            "---------------------------------------\n",
            "Total Timesteps: 266000 Episode Num: 266 Reward: 1913.7900901197304\n",
            "Total Timesteps: 267000 Episode Num: 267 Reward: 1854.3094109795425\n",
            "Total Timesteps: 268000 Episode Num: 268 Reward: 1889.9599899988668\n",
            "Total Timesteps: 269000 Episode Num: 269 Reward: 1758.022060664776\n",
            "Total Timesteps: 270000 Episode Num: 270 Reward: 1869.7199977498408\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1849.622351\n",
            "---------------------------------------\n",
            "Total Timesteps: 271000 Episode Num: 271 Reward: 1862.8488371211185\n",
            "Total Timesteps: 272000 Episode Num: 272 Reward: 1936.403415829218\n",
            "Total Timesteps: 273000 Episode Num: 273 Reward: 1941.6172730635951\n",
            "Total Timesteps: 274000 Episode Num: 274 Reward: 1538.9912507681843\n",
            "Total Timesteps: 275000 Episode Num: 275 Reward: 1918.3088851338518\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1931.603240\n",
            "---------------------------------------\n",
            "Total Timesteps: 276000 Episode Num: 276 Reward: 1917.5430746844963\n",
            "Total Timesteps: 277000 Episode Num: 277 Reward: 1954.8355445651457\n",
            "Total Timesteps: 278000 Episode Num: 278 Reward: 1866.4041056083988\n",
            "Total Timesteps: 279000 Episode Num: 279 Reward: 1960.4480154065452\n",
            "Total Timesteps: 280000 Episode Num: 280 Reward: 1921.7122706436276\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1971.922414\n",
            "---------------------------------------\n",
            "Total Timesteps: 281000 Episode Num: 281 Reward: 1954.7240765774238\n",
            "Total Timesteps: 282000 Episode Num: 282 Reward: 1944.651148514868\n",
            "Total Timesteps: 283000 Episode Num: 283 Reward: 1926.3721546043266\n",
            "Total Timesteps: 284000 Episode Num: 284 Reward: 1898.5106256400238\n",
            "Total Timesteps: 285000 Episode Num: 285 Reward: 1875.9010227688623\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1869.161949\n",
            "---------------------------------------\n",
            "Total Timesteps: 286000 Episode Num: 286 Reward: 1892.3608069843112\n",
            "Total Timesteps: 287000 Episode Num: 287 Reward: 1843.9530842737774\n",
            "Total Timesteps: 288000 Episode Num: 288 Reward: 1906.152301575465\n",
            "Total Timesteps: 289000 Episode Num: 289 Reward: 1890.5313156608902\n",
            "Total Timesteps: 290000 Episode Num: 290 Reward: 1965.1705225029987\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1940.696856\n",
            "---------------------------------------\n",
            "Total Timesteps: 291000 Episode Num: 291 Reward: 1897.2745876240808\n",
            "Total Timesteps: 292000 Episode Num: 292 Reward: 1912.6768367526224\n",
            "Total Timesteps: 293000 Episode Num: 293 Reward: 2012.3244466860174\n",
            "Total Timesteps: 294000 Episode Num: 294 Reward: 1955.7421428688363\n",
            "Total Timesteps: 295000 Episode Num: 295 Reward: 1990.126752182147\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2080.580611\n",
            "---------------------------------------\n",
            "Total Timesteps: 296000 Episode Num: 296 Reward: 2036.7288599092756\n",
            "Total Timesteps: 297000 Episode Num: 297 Reward: 1930.9202528634833\n",
            "Total Timesteps: 298000 Episode Num: 298 Reward: 1980.814255358777\n",
            "Total Timesteps: 299000 Episode Num: 299 Reward: 1976.150384282775\n",
            "Total Timesteps: 300000 Episode Num: 300 Reward: 2034.6160907803835\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1984.430788\n",
            "---------------------------------------\n",
            "Total Timesteps: 301000 Episode Num: 301 Reward: 1944.2851691701978\n",
            "Total Timesteps: 302000 Episode Num: 302 Reward: 2015.8355684157968\n",
            "Total Timesteps: 303000 Episode Num: 303 Reward: 1908.2451279660484\n",
            "Total Timesteps: 304000 Episode Num: 304 Reward: 1977.0625882214276\n",
            "Total Timesteps: 305000 Episode Num: 305 Reward: 1945.7693353354928\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1971.808201\n",
            "---------------------------------------\n",
            "Total Timesteps: 306000 Episode Num: 306 Reward: 1912.0871109946122\n",
            "Total Timesteps: 307000 Episode Num: 307 Reward: 1975.2658956999633\n",
            "Total Timesteps: 308000 Episode Num: 308 Reward: 1976.4621669788796\n",
            "Total Timesteps: 309000 Episode Num: 309 Reward: 2026.5650039340599\n",
            "Total Timesteps: 310000 Episode Num: 310 Reward: 1985.0359055171577\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1955.227698\n",
            "---------------------------------------\n",
            "Total Timesteps: 311000 Episode Num: 311 Reward: 1911.3395305607783\n",
            "Total Timesteps: 312000 Episode Num: 312 Reward: 2026.9070022128276\n",
            "Total Timesteps: 313000 Episode Num: 313 Reward: 1982.5281715715726\n",
            "Total Timesteps: 314000 Episode Num: 314 Reward: 1924.9305591581335\n",
            "Total Timesteps: 315000 Episode Num: 315 Reward: 1921.9657879250883\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2034.361546\n",
            "---------------------------------------\n",
            "Total Timesteps: 316000 Episode Num: 316 Reward: 1965.1755343683942\n",
            "Total Timesteps: 317000 Episode Num: 317 Reward: 1937.7067271852484\n",
            "Total Timesteps: 318000 Episode Num: 318 Reward: 2027.8461087208505\n",
            "Total Timesteps: 319000 Episode Num: 319 Reward: 2037.6268098507032\n",
            "Total Timesteps: 320000 Episode Num: 320 Reward: 1944.331991145624\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2014.534645\n",
            "---------------------------------------\n",
            "Total Timesteps: 321000 Episode Num: 321 Reward: 1975.4529382541352\n",
            "Total Timesteps: 322000 Episode Num: 322 Reward: 2045.5211040604904\n",
            "Total Timesteps: 323000 Episode Num: 323 Reward: 1995.8735348667506\n",
            "Total Timesteps: 324000 Episode Num: 324 Reward: 1949.0115244092683\n",
            "Total Timesteps: 325000 Episode Num: 325 Reward: 1978.2969364446915\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1998.927658\n",
            "---------------------------------------\n",
            "Total Timesteps: 326000 Episode Num: 326 Reward: 1956.132624684449\n",
            "Total Timesteps: 327000 Episode Num: 327 Reward: 1985.9096055630223\n",
            "Total Timesteps: 328000 Episode Num: 328 Reward: 2045.25366033149\n",
            "Total Timesteps: 329000 Episode Num: 329 Reward: 1969.9609027364972\n",
            "Total Timesteps: 330000 Episode Num: 330 Reward: 1947.5185483085008\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2080.756422\n",
            "---------------------------------------\n",
            "Total Timesteps: 331000 Episode Num: 331 Reward: 2035.0014461083147\n",
            "Total Timesteps: 332000 Episode Num: 332 Reward: 2003.8124790538775\n",
            "Total Timesteps: 333000 Episode Num: 333 Reward: 1919.9272841524148\n",
            "Total Timesteps: 334000 Episode Num: 334 Reward: 1961.6316453575014\n",
            "Total Timesteps: 335000 Episode Num: 335 Reward: 1974.3740416105634\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2019.138193\n",
            "---------------------------------------\n",
            "Total Timesteps: 336000 Episode Num: 336 Reward: 2041.3364102204239\n",
            "Total Timesteps: 337000 Episode Num: 337 Reward: 1977.3996318189897\n",
            "Total Timesteps: 338000 Episode Num: 338 Reward: 1985.267375290728\n",
            "Total Timesteps: 339000 Episode Num: 339 Reward: 1997.90355167223\n",
            "Total Timesteps: 340000 Episode Num: 340 Reward: 1985.5855465954894\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1951.884930\n",
            "---------------------------------------\n",
            "Total Timesteps: 341000 Episode Num: 341 Reward: 1897.7507355935766\n",
            "Total Timesteps: 342000 Episode Num: 342 Reward: 2006.9678593071703\n",
            "Total Timesteps: 343000 Episode Num: 343 Reward: 2012.2612078033167\n",
            "Total Timesteps: 344000 Episode Num: 344 Reward: 2109.428822656578\n",
            "Total Timesteps: 345000 Episode Num: 345 Reward: 2087.9947131617446\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2037.375504\n",
            "---------------------------------------\n",
            "Total Timesteps: 346000 Episode Num: 346 Reward: 1986.0994517669285\n",
            "Total Timesteps: 347000 Episode Num: 347 Reward: 2051.273417637855\n",
            "Total Timesteps: 348000 Episode Num: 348 Reward: 2022.4853512725479\n",
            "Total Timesteps: 349000 Episode Num: 349 Reward: 2034.1215422860282\n",
            "Total Timesteps: 350000 Episode Num: 350 Reward: 1814.5296232966612\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2086.895726\n",
            "---------------------------------------\n",
            "Total Timesteps: 351000 Episode Num: 351 Reward: 2026.3699570215974\n",
            "Total Timesteps: 352000 Episode Num: 352 Reward: 2147.7607138455833\n",
            "Total Timesteps: 353000 Episode Num: 353 Reward: 2168.5185461458645\n",
            "Total Timesteps: 354000 Episode Num: 354 Reward: 1947.2784594075895\n",
            "Total Timesteps: 355000 Episode Num: 355 Reward: 2147.31155502955\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2113.956485\n",
            "---------------------------------------\n",
            "Total Timesteps: 356000 Episode Num: 356 Reward: 2079.513656569933\n",
            "Total Timesteps: 357000 Episode Num: 357 Reward: 2032.8080610391582\n",
            "Total Timesteps: 358000 Episode Num: 358 Reward: 2059.7477587363837\n",
            "Total Timesteps: 359000 Episode Num: 359 Reward: 2141.3681731679926\n",
            "Total Timesteps: 360000 Episode Num: 360 Reward: 2038.3635981639966\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2045.319886\n",
            "---------------------------------------\n",
            "Total Timesteps: 361000 Episode Num: 361 Reward: 1951.7046465621243\n",
            "Total Timesteps: 362000 Episode Num: 362 Reward: 2108.1173009978156\n",
            "Total Timesteps: 363000 Episode Num: 363 Reward: 2107.7091963875814\n",
            "Total Timesteps: 364000 Episode Num: 364 Reward: 2181.9501789878336\n",
            "Total Timesteps: 365000 Episode Num: 365 Reward: 2123.150568700504\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2111.007592\n",
            "---------------------------------------\n",
            "Total Timesteps: 366000 Episode Num: 366 Reward: 2059.0088844407255\n",
            "Total Timesteps: 367000 Episode Num: 367 Reward: 2104.635377058732\n",
            "Total Timesteps: 368000 Episode Num: 368 Reward: 2118.265935090551\n",
            "Total Timesteps: 369000 Episode Num: 369 Reward: 2150.4027387188776\n",
            "Total Timesteps: 370000 Episode Num: 370 Reward: 2136.2428564977154\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2144.924852\n",
            "---------------------------------------\n",
            "Total Timesteps: 371000 Episode Num: 371 Reward: 2085.5942273482037\n",
            "Total Timesteps: 372000 Episode Num: 372 Reward: 2036.125324085993\n",
            "Total Timesteps: 373000 Episode Num: 373 Reward: 2110.15456806413\n",
            "Total Timesteps: 374000 Episode Num: 374 Reward: 2118.321675157868\n",
            "Total Timesteps: 375000 Episode Num: 375 Reward: 2054.040421308217\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2136.059065\n",
            "---------------------------------------\n",
            "Total Timesteps: 376000 Episode Num: 376 Reward: 2109.453109781204\n",
            "Total Timesteps: 377000 Episode Num: 377 Reward: 2146.379174233245\n",
            "Total Timesteps: 378000 Episode Num: 378 Reward: 2052.2455500171004\n",
            "Total Timesteps: 379000 Episode Num: 379 Reward: 2061.422921196104\n",
            "Total Timesteps: 380000 Episode Num: 380 Reward: 2095.8494676561572\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2168.424399\n",
            "---------------------------------------\n",
            "Total Timesteps: 381000 Episode Num: 381 Reward: 2159.5441635839074\n",
            "Total Timesteps: 382000 Episode Num: 382 Reward: 2118.436456434611\n",
            "Total Timesteps: 383000 Episode Num: 383 Reward: 2097.7194855133\n",
            "Total Timesteps: 384000 Episode Num: 384 Reward: 2147.4666282567237\n",
            "Total Timesteps: 385000 Episode Num: 385 Reward: 2096.519265972879\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2164.259269\n",
            "---------------------------------------\n",
            "Total Timesteps: 386000 Episode Num: 386 Reward: 2160.8989341362717\n",
            "Total Timesteps: 387000 Episode Num: 387 Reward: 2103.863067017289\n",
            "Total Timesteps: 388000 Episode Num: 388 Reward: 2158.365927177557\n",
            "Total Timesteps: 389000 Episode Num: 389 Reward: 2188.9909819890568\n",
            "Total Timesteps: 390000 Episode Num: 390 Reward: 2211.2558712139316\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2158.243987\n",
            "---------------------------------------\n",
            "Total Timesteps: 391000 Episode Num: 391 Reward: 1902.6946478681539\n",
            "Total Timesteps: 392000 Episode Num: 392 Reward: 2235.020811836715\n",
            "Total Timesteps: 393000 Episode Num: 393 Reward: 2163.510269374453\n",
            "Total Timesteps: 394000 Episode Num: 394 Reward: 2122.9962534580454\n",
            "Total Timesteps: 395000 Episode Num: 395 Reward: 2193.9892680303697\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2191.708599\n",
            "---------------------------------------\n",
            "Total Timesteps: 396000 Episode Num: 396 Reward: 2118.4347417521894\n",
            "Total Timesteps: 397000 Episode Num: 397 Reward: 2157.278963863128\n",
            "Total Timesteps: 398000 Episode Num: 398 Reward: 2095.467745215321\n",
            "Total Timesteps: 399000 Episode Num: 399 Reward: 2086.9584949798555\n",
            "Total Timesteps: 400000 Episode Num: 400 Reward: 2127.5848668442272\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2258.762301\n",
            "---------------------------------------\n",
            "Total Timesteps: 401000 Episode Num: 401 Reward: 2166.8282099405405\n",
            "Total Timesteps: 402000 Episode Num: 402 Reward: 2152.45559434993\n",
            "Total Timesteps: 403000 Episode Num: 403 Reward: 2097.436804917061\n",
            "Total Timesteps: 404000 Episode Num: 404 Reward: 2213.5860580365415\n",
            "Total Timesteps: 405000 Episode Num: 405 Reward: 2216.127755780416\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2183.270011\n",
            "---------------------------------------\n",
            "Total Timesteps: 406000 Episode Num: 406 Reward: 2192.456269654215\n",
            "Total Timesteps: 407000 Episode Num: 407 Reward: 2223.1532693800004\n",
            "Total Timesteps: 408000 Episode Num: 408 Reward: 2117.0241741871496\n",
            "Total Timesteps: 409000 Episode Num: 409 Reward: 2165.7257462790267\n",
            "Total Timesteps: 410000 Episode Num: 410 Reward: 2204.786361518688\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2195.221851\n",
            "---------------------------------------\n",
            "Total Timesteps: 411000 Episode Num: 411 Reward: 2142.2420575002197\n",
            "Total Timesteps: 412000 Episode Num: 412 Reward: 2145.045017214866\n",
            "Total Timesteps: 413000 Episode Num: 413 Reward: 2111.9671567136033\n",
            "Total Timesteps: 414000 Episode Num: 414 Reward: 2085.6327069352833\n",
            "Total Timesteps: 415000 Episode Num: 415 Reward: 2188.1037246851224\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2189.904740\n",
            "---------------------------------------\n",
            "Total Timesteps: 416000 Episode Num: 416 Reward: 2111.40390051726\n",
            "Total Timesteps: 417000 Episode Num: 417 Reward: 2058.293100887662\n",
            "Total Timesteps: 418000 Episode Num: 418 Reward: 2119.103055083001\n",
            "Total Timesteps: 419000 Episode Num: 419 Reward: 2073.427992237257\n",
            "Total Timesteps: 420000 Episode Num: 420 Reward: 2180.4224480731536\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2228.615897\n",
            "---------------------------------------\n",
            "Total Timesteps: 421000 Episode Num: 421 Reward: 2097.3542907020906\n",
            "Total Timesteps: 422000 Episode Num: 422 Reward: 2153.800428681796\n",
            "Total Timesteps: 423000 Episode Num: 423 Reward: 2185.972739459919\n",
            "Total Timesteps: 424000 Episode Num: 424 Reward: 2189.806011549781\n",
            "Total Timesteps: 425000 Episode Num: 425 Reward: 2091.324221171977\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2031.076802\n",
            "---------------------------------------\n",
            "Total Timesteps: 426000 Episode Num: 426 Reward: 2007.563441027129\n",
            "Total Timesteps: 427000 Episode Num: 427 Reward: 2235.202770215885\n",
            "Total Timesteps: 428000 Episode Num: 428 Reward: 2235.6002856915998\n",
            "Total Timesteps: 429000 Episode Num: 429 Reward: 2195.5934381315615\n",
            "Total Timesteps: 430000 Episode Num: 430 Reward: 2184.0914697193543\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2230.757364\n",
            "---------------------------------------\n",
            "Total Timesteps: 431000 Episode Num: 431 Reward: 2184.9427033778948\n",
            "Total Timesteps: 432000 Episode Num: 432 Reward: 2177.974101741306\n",
            "Total Timesteps: 433000 Episode Num: 433 Reward: 2256.089376333285\n",
            "Total Timesteps: 434000 Episode Num: 434 Reward: 2242.725491878074\n",
            "Total Timesteps: 435000 Episode Num: 435 Reward: 2271.3154235594125\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2228.936198\n",
            "---------------------------------------\n",
            "Total Timesteps: 436000 Episode Num: 436 Reward: 2197.167529417228\n",
            "Total Timesteps: 437000 Episode Num: 437 Reward: 2289.617405814355\n",
            "Total Timesteps: 438000 Episode Num: 438 Reward: 2209.1044684890867\n",
            "Total Timesteps: 439000 Episode Num: 439 Reward: 2166.9506859241637\n",
            "Total Timesteps: 440000 Episode Num: 440 Reward: 2215.533765076855\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2260.234488\n",
            "---------------------------------------\n",
            "Total Timesteps: 441000 Episode Num: 441 Reward: 2194.5197176612187\n",
            "Total Timesteps: 442000 Episode Num: 442 Reward: 2206.318160525416\n",
            "Total Timesteps: 443000 Episode Num: 443 Reward: 2244.859736421028\n",
            "Total Timesteps: 444000 Episode Num: 444 Reward: 2155.6682343838884\n",
            "Total Timesteps: 445000 Episode Num: 445 Reward: 2198.2824787639547\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2079.177155\n",
            "---------------------------------------\n",
            "Total Timesteps: 446000 Episode Num: 446 Reward: 1998.3856413633955\n",
            "Total Timesteps: 447000 Episode Num: 447 Reward: 2213.391065230635\n",
            "Total Timesteps: 448000 Episode Num: 448 Reward: 2213.3544584714778\n",
            "Total Timesteps: 449000 Episode Num: 449 Reward: 2230.1622863695206\n",
            "Total Timesteps: 450000 Episode Num: 450 Reward: 2214.7375234783776\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2319.853898\n",
            "---------------------------------------\n",
            "Total Timesteps: 451000 Episode Num: 451 Reward: 2215.8799439561453\n",
            "Total Timesteps: 452000 Episode Num: 452 Reward: 2207.316292570937\n",
            "Total Timesteps: 453000 Episode Num: 453 Reward: 2156.6636193721456\n",
            "Total Timesteps: 454000 Episode Num: 454 Reward: 2241.969667304623\n",
            "Total Timesteps: 455000 Episode Num: 455 Reward: 2178.4848685061315\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2277.247987\n",
            "---------------------------------------\n",
            "Total Timesteps: 456000 Episode Num: 456 Reward: 2242.1260562677903\n",
            "Total Timesteps: 457000 Episode Num: 457 Reward: 2171.4650440275163\n",
            "Total Timesteps: 458000 Episode Num: 458 Reward: 2276.969591090381\n",
            "Total Timesteps: 459000 Episode Num: 459 Reward: 2133.009245350481\n",
            "Total Timesteps: 460000 Episode Num: 460 Reward: 2194.289062859736\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2247.298541\n",
            "---------------------------------------\n",
            "Total Timesteps: 461000 Episode Num: 461 Reward: 2179.41312336557\n",
            "Total Timesteps: 462000 Episode Num: 462 Reward: 2286.0594300244347\n",
            "Total Timesteps: 463000 Episode Num: 463 Reward: 2226.544898507677\n",
            "Total Timesteps: 464000 Episode Num: 464 Reward: 2244.961661111021\n",
            "Total Timesteps: 465000 Episode Num: 465 Reward: 2219.71594494328\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2348.141532\n",
            "---------------------------------------\n",
            "Total Timesteps: 466000 Episode Num: 466 Reward: 2284.424564967737\n",
            "Total Timesteps: 467000 Episode Num: 467 Reward: 2205.0251189154255\n",
            "Total Timesteps: 468000 Episode Num: 468 Reward: 2254.7336203878126\n",
            "Total Timesteps: 469000 Episode Num: 469 Reward: 2274.1673995919136\n",
            "Total Timesteps: 470000 Episode Num: 470 Reward: 2244.390224570319\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2306.154602\n",
            "---------------------------------------\n",
            "Total Timesteps: 471000 Episode Num: 471 Reward: 2268.368016678662\n",
            "Total Timesteps: 472000 Episode Num: 472 Reward: 2199.645466368961\n",
            "Total Timesteps: 473000 Episode Num: 473 Reward: 2264.054224447942\n",
            "Total Timesteps: 474000 Episode Num: 474 Reward: 2311.956146143691\n",
            "Total Timesteps: 475000 Episode Num: 475 Reward: 2257.0358933360058\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1925.574049\n",
            "---------------------------------------\n",
            "Total Timesteps: 476000 Episode Num: 476 Reward: 2279.8367546118648\n",
            "Total Timesteps: 477000 Episode Num: 477 Reward: 2240.520126496511\n",
            "Total Timesteps: 478000 Episode Num: 478 Reward: 2264.9365332030125\n",
            "Total Timesteps: 479000 Episode Num: 479 Reward: -600.6168012755911\n",
            "Total Timesteps: 480000 Episode Num: 480 Reward: 1130.4309612297864\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1653.258836\n",
            "---------------------------------------\n",
            "Total Timesteps: 481000 Episode Num: 481 Reward: 2157.822356850445\n",
            "Total Timesteps: 482000 Episode Num: 482 Reward: 2262.2308305302035\n",
            "Total Timesteps: 483000 Episode Num: 483 Reward: 2256.0265970756927\n",
            "Total Timesteps: 484000 Episode Num: 484 Reward: 2261.4925367584124\n",
            "Total Timesteps: 485000 Episode Num: 485 Reward: 2277.147231700578\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2344.796453\n",
            "---------------------------------------\n",
            "Total Timesteps: 486000 Episode Num: 486 Reward: 2279.3183435346173\n",
            "Total Timesteps: 487000 Episode Num: 487 Reward: 2282.063021375686\n",
            "Total Timesteps: 488000 Episode Num: 488 Reward: 2170.9927974752404\n",
            "Total Timesteps: 489000 Episode Num: 489 Reward: 2235.8718488811755\n",
            "Total Timesteps: 490000 Episode Num: 490 Reward: 2262.617265393424\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2058.520252\n",
            "---------------------------------------\n",
            "Total Timesteps: 491000 Episode Num: 491 Reward: 2329.2240165275825\n",
            "Total Timesteps: 492000 Episode Num: 492 Reward: 2262.7584824801934\n",
            "Total Timesteps: 493000 Episode Num: 493 Reward: 2272.956612677239\n",
            "Total Timesteps: 494000 Episode Num: 494 Reward: 2304.9170208510363\n",
            "Total Timesteps: 495000 Episode Num: 495 Reward: 2260.3754920274964\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2285.569341\n",
            "---------------------------------------\n",
            "Total Timesteps: 496000 Episode Num: 496 Reward: 2244.420062975833\n",
            "Total Timesteps: 497000 Episode Num: 497 Reward: 2220.2089142063423\n",
            "Total Timesteps: 498000 Episode Num: 498 Reward: 2215.263348385642\n",
            "Total Timesteps: 499000 Episode Num: 499 Reward: 2262.575834775798\n",
            "Total Timesteps: 500000 Episode Num: 500 Reward: 2261.4439567279333\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2361.797767\n",
            "---------------------------------------\n",
            "Total Timesteps: 501000 Episode Num: 501 Reward: 2299.3707837312268\n",
            "Total Timesteps: 502000 Episode Num: 502 Reward: 2269.806424429194\n",
            "Total Timesteps: 503000 Episode Num: 503 Reward: 2256.718088675919\n",
            "Total Timesteps: 504000 Episode Num: 504 Reward: 2267.304786673272\n",
            "Total Timesteps: 505000 Episode Num: 505 Reward: 2151.306097834472\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2347.393654\n",
            "---------------------------------------\n",
            "Total Timesteps: 506000 Episode Num: 506 Reward: 2293.1555858294305\n",
            "Total Timesteps: 507000 Episode Num: 507 Reward: 2221.647760544468\n",
            "Total Timesteps: 508000 Episode Num: 508 Reward: 2174.7457660238756\n",
            "Total Timesteps: 509000 Episode Num: 509 Reward: 2354.7757144633133\n",
            "Total Timesteps: 510000 Episode Num: 510 Reward: 2256.340242367201\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2404.442380\n",
            "---------------------------------------\n",
            "Total Timesteps: 511000 Episode Num: 511 Reward: 2314.6165371875113\n",
            "Total Timesteps: 512000 Episode Num: 512 Reward: 2284.6180196601154\n",
            "Total Timesteps: 513000 Episode Num: 513 Reward: 2321.918962029439\n",
            "Total Timesteps: 514000 Episode Num: 514 Reward: 2361.2730201298386\n",
            "Total Timesteps: 515000 Episode Num: 515 Reward: 2271.60780238904\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2393.553160\n",
            "---------------------------------------\n",
            "Total Timesteps: 516000 Episode Num: 516 Reward: 2326.31528096937\n",
            "Total Timesteps: 517000 Episode Num: 517 Reward: 2361.0920646691093\n",
            "Total Timesteps: 518000 Episode Num: 518 Reward: 2281.982997404302\n",
            "Total Timesteps: 519000 Episode Num: 519 Reward: 2268.5249383893483\n",
            "Total Timesteps: 520000 Episode Num: 520 Reward: 2289.974538457974\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2389.813347\n",
            "---------------------------------------\n",
            "Total Timesteps: 521000 Episode Num: 521 Reward: 2324.8963291452123\n",
            "Total Timesteps: 522000 Episode Num: 522 Reward: 2317.574399770194\n",
            "Total Timesteps: 523000 Episode Num: 523 Reward: 2391.719642528325\n",
            "Total Timesteps: 524000 Episode Num: 524 Reward: 2351.4671842388557\n",
            "Total Timesteps: 525000 Episode Num: 525 Reward: 2365.359160094523\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2388.899270\n",
            "---------------------------------------\n",
            "Total Timesteps: 526000 Episode Num: 526 Reward: 2302.6885435058884\n",
            "Total Timesteps: 527000 Episode Num: 527 Reward: 2299.9622843920874\n",
            "Total Timesteps: 528000 Episode Num: 528 Reward: 2312.4785845409333\n",
            "Total Timesteps: 529000 Episode Num: 529 Reward: 2314.940445450696\n",
            "Total Timesteps: 530000 Episode Num: 530 Reward: 2365.2171500512145\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2426.360582\n",
            "---------------------------------------\n",
            "Total Timesteps: 531000 Episode Num: 531 Reward: 2334.2627032155915\n",
            "Total Timesteps: 532000 Episode Num: 532 Reward: 2321.2690751716177\n",
            "Total Timesteps: 533000 Episode Num: 533 Reward: 2320.6522910084595\n",
            "Total Timesteps: 534000 Episode Num: 534 Reward: 2274.9153188385444\n",
            "Total Timesteps: 535000 Episode Num: 535 Reward: 2272.0592684906724\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2374.072767\n",
            "---------------------------------------\n",
            "Total Timesteps: 536000 Episode Num: 536 Reward: 2356.4039106849464\n",
            "Total Timesteps: 537000 Episode Num: 537 Reward: 2324.7243541601442\n",
            "Total Timesteps: 538000 Episode Num: 538 Reward: 2350.5204024335712\n",
            "Total Timesteps: 539000 Episode Num: 539 Reward: 2275.9530272684415\n",
            "Total Timesteps: 540000 Episode Num: 540 Reward: 2332.6072295156073\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2370.663695\n",
            "---------------------------------------\n",
            "Total Timesteps: 541000 Episode Num: 541 Reward: 2309.1881567422392\n",
            "Total Timesteps: 542000 Episode Num: 542 Reward: 2367.5024703067943\n",
            "Total Timesteps: 543000 Episode Num: 543 Reward: 2377.753920497017\n",
            "Total Timesteps: 544000 Episode Num: 544 Reward: 2375.11840919738\n",
            "Total Timesteps: 545000 Episode Num: 545 Reward: 2339.4941528284066\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2407.391716\n",
            "---------------------------------------\n",
            "Total Timesteps: 546000 Episode Num: 546 Reward: 2385.712675972014\n",
            "Total Timesteps: 547000 Episode Num: 547 Reward: 2216.929324497616\n",
            "Total Timesteps: 548000 Episode Num: 548 Reward: 2322.598775501112\n",
            "Total Timesteps: 549000 Episode Num: 549 Reward: 2325.4993921920077\n",
            "Total Timesteps: 550000 Episode Num: 550 Reward: 1979.221097340024\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2424.294984\n",
            "---------------------------------------\n",
            "Total Timesteps: 551000 Episode Num: 551 Reward: 2377.4347867025167\n",
            "Total Timesteps: 552000 Episode Num: 552 Reward: 2337.058687037141\n",
            "Total Timesteps: 553000 Episode Num: 553 Reward: 2284.9725614077083\n",
            "Total Timesteps: 554000 Episode Num: 554 Reward: 2008.4479045250766\n",
            "Total Timesteps: 555000 Episode Num: 555 Reward: 2413.1766419389623\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2391.420619\n",
            "---------------------------------------\n",
            "Total Timesteps: 556000 Episode Num: 556 Reward: 2443.256340510959\n",
            "Total Timesteps: 557000 Episode Num: 557 Reward: 2384.533093452796\n",
            "Total Timesteps: 558000 Episode Num: 558 Reward: 2311.852874538319\n",
            "Total Timesteps: 559000 Episode Num: 559 Reward: 2262.0677698653144\n",
            "Total Timesteps: 560000 Episode Num: 560 Reward: 2417.071728916918\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2467.881015\n",
            "---------------------------------------\n",
            "Total Timesteps: 561000 Episode Num: 561 Reward: 2486.6070974787285\n",
            "Total Timesteps: 562000 Episode Num: 562 Reward: 2364.2485388572586\n",
            "Total Timesteps: 563000 Episode Num: 563 Reward: 2372.65004618772\n",
            "Total Timesteps: 564000 Episode Num: 564 Reward: 2278.68673290727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4204bed03ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total Timesteps: {} Episode Num: {} Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0;31m#print(\"episode_timesteps:\",episode_timesteps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-26d387db6b38>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0;31m# Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "61959791-1632-4e2d-c029-da9f24fc4395"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"HalfCheetahBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2464.275637\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcSNoQj3b7eI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a46c404f-75d4-43f9-ddf0-4ab3d86f0ffb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypdIkehEb9ti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e97aee9-1b8a-422d-ca25-4e662ca50c43"
      },
      "source": [
        "cd /content/drive/\"My Drive\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qM9CsN2cMEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir Reiforcement_Outputs_Cheetah"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZLk16olch77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e22370a-13ff-48f9-a097-af7a5bc619ee"
      },
      "source": [
        "cd Reiforcement_Outputs_Cheetah"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Reiforcement_Outputs_Cheetah\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVEv1QMBckq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/exp/brs/monitor/openaigym.video.1.2851.video000001.mp4 /content/drive/\"My Drive\"/Reiforcement_Outputs_Cheetah"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_omL6SadJGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/pytorch_models/TD3_HalfCheetahBulletEnv-v0_0_actor.pth /content/drive/\"My Drive\"/Reiforcement_Outputs_Cheetah"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQJu_bd3dMa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/pytorch_models/TD3_HalfCheetahBulletEnv-v0_0_critic.pth /content/drive/\"My Drive\"/Reiforcement_Outputs_Cheetah"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}